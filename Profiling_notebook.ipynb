{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ipc9K-g9i2Ls"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Fatim_Sproj\\anaconda3\\envs\\bacp\\lib\\site-packages\\torch\\cuda\\__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.profiler import profile, ProfilerActivity\n",
        "from torchprofile import profile_macs\n",
        "\n",
        "try:\n",
        "    from pyJoules.energy_meter import measure_energy\n",
        "    from pyJoules.device.rapl_device import RaplPackageDomain, RaplDramDomain\n",
        "    from pyJoules.device.nvidia_device import NvidiaGPUDomain\n",
        "    PYJOULES_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PYJOULES_AVAILABLE = False\n",
        "\n",
        "\n",
        "def get_model_size_mb(model, temp_path=\"temp_model.pt\"):\n",
        "    torch.save(model.state_dict(), temp_path)\n",
        "    size_mb = os.path.getsize(temp_path) / (1024 ** 2)\n",
        "    os.remove(temp_path)\n",
        "    return round(size_mb, 3)\n",
        "\n",
        "def get_file_size_mb(path):\n",
        "    return round(os.path.getsize(path) / (1024 ** 2), 3)\n",
        "\n",
        "def get_macs(model, batch_size=1, img_size=None, device=\"cuda\", dataset_name=None):\n",
        "    if img_size is None:\n",
        "        if dataset_name in [\"cifar10\", \"cifar100\", \"rmnist\"]:\n",
        "            img_size = 32\n",
        "        else:\n",
        "            img_size = 224\n",
        "    dummy_input = torch.randn(batch_size, 3, img_size, img_size).to(device)\n",
        "    macs = profile_macs(model, dummy_input)\n",
        "    return round(macs / 1e6, 3)\n",
        "\n",
        "def measure_inference_latency(model, dataloader, device, num_batches=10):\n",
        "    model.eval()\n",
        "    latencies, peak_mem, avg_mem = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, _) in enumerate(dataloader):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "            inputs = inputs.to(device)\n",
        "            torch.cuda.reset_peak_memory_stats(device)\n",
        "            torch.cuda.synchronize()\n",
        "            start = time.perf_counter()\n",
        "            with profile(activities=[ProfilerActivity.CUDA, ProfilerActivity.CPU],\n",
        "                         record_shapes=False, profile_memory=True) as prof:\n",
        "                outputs = model(inputs)\n",
        "            torch.cuda.synchronize()\n",
        "            end = time.perf_counter()\n",
        "            latency = (end - start) * 1000\n",
        "            latencies.append(latency)\n",
        "            peak_mem.append(torch.cuda.max_memory_allocated(device) / (1024 ** 2))\n",
        "            avg_mem.append(torch.cuda.memory_allocated(device) / (1024 ** 2))\n",
        "    avg_latency = round(sum(latencies) / len(latencies), 3)\n",
        "    avg_peak_mem = round(sum(peak_mem) / len(peak_mem), 3)\n",
        "    avg_used_mem = round(sum(avg_mem) / len(avg_mem), 3)\n",
        "    return avg_latency, avg_peak_mem, avg_used_mem\n",
        "\n",
        "def evaluate_accuracy(model, dataloader, device, topk=(1, 5)):\n",
        "    model.eval()\n",
        "    correct_top1, correct_top5, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, pred_top1 = outputs.max(1)\n",
        "            correct_top1 += pred_top1.eq(labels).sum().item()\n",
        "            _, pred_top5 = outputs.topk(5, 1, True, True)\n",
        "            correct_top5 += sum([labels[i] in pred_top5[i] for i in range(len(labels))])\n",
        "            total += labels.size(0)\n",
        "    top1 = 100.0 * correct_top1 / total\n",
        "    top5 = 100.0 * correct_top5 / total\n",
        "    return round(top1, 3), round(top5, 3)\n",
        "\n",
        "def get_gpu_power(device_id=0):\n",
        "    try:\n",
        "        output = subprocess.check_output(\n",
        "            f\"nvidia-smi --query-gpu=power.draw --format=csv,noheader,nounits -i {device_id}\",\n",
        "            shell=True\n",
        "        ).decode(\"utf-8\").strip()\n",
        "        return float(output)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def measure_avg_gpu_power(model, dataloader, device, num_batches=10, device_id=0):\n",
        "    model.eval()\n",
        "    powers = []\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, _) in enumerate(dataloader):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "            inputs = inputs.to(device)\n",
        "            _ = model(inputs)\n",
        "            p = get_gpu_power(device_id)\n",
        "            if p:\n",
        "                powers.append(p)\n",
        "    if not powers:\n",
        "        return None\n",
        "    avg_power = sum(powers) / len(powers)\n",
        "    avg_energy_mJ = avg_power * (num_batches * 0.1) * 1000\n",
        "    return round(avg_energy_mJ, 3)\n",
        "\n",
        "def measure_energy_model(model, dataloader, device, num_batches=10):\n",
        "    if not PYJOULES_AVAILABLE:\n",
        "        return None\n",
        "    energy_records = []\n",
        "    try:\n",
        "        @measure_energy(domains=[RaplPackageDomain(0), RaplDramDomain(0), NvidiaGPUDomain(0)])\n",
        "        def inference_batch(inputs):\n",
        "            with torch.no_grad():\n",
        "                _ = model(inputs)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (inputs, _) in enumerate(dataloader):\n",
        "                if i >= num_batches:\n",
        "                    break\n",
        "                inputs = inputs.to(device)\n",
        "                inference_batch(inputs)\n",
        "                energy_records.append(getattr(inference_batch, \"energy_consumed\", None))\n",
        "        energy_vals = [e for e in energy_records if e is not None]\n",
        "        if len(energy_vals) == 0:\n",
        "            return None\n",
        "        avg_energy = sum(energy_vals) / len(energy_vals)\n",
        "        return round(avg_energy, 3)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def profile_model(model, dataloader, dataset_name, device=\"cuda\", num_batches=10,\n",
        "                  model_path_for_size=None, precomputed_macs_m=None):\n",
        "    print(f\"\\nProfiling {model.__class__.__name__} on {dataset_name}...\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    if model_path_for_size:\n",
        "        size_mb = get_file_size_mb(model_path_for_size)\n",
        "    else:\n",
        "        size_mb = get_model_size_mb(model)\n",
        "    if precomputed_macs_m:\n",
        "        macs_million = precomputed_macs_m\n",
        "    else:\n",
        "        macs_million = get_macs(model, batch_size=1, device=device, dataset_name=dataset_name)\n",
        "    latency_ms, peak_mem_mb, avg_mem_mb = measure_inference_latency(\n",
        "        model, dataloader, device, num_batches\n",
        "    )\n",
        "    energy_mj = measure_energy_model(model, dataloader, device, num_batches)\n",
        "    if energy_mj is None:\n",
        "        energy_mj = measure_avg_gpu_power(model, dataloader, device, num_batches)\n",
        "    top1, top5 = evaluate_accuracy(model, dataloader, device)\n",
        "    results = {\n",
        "        \"Model\": model.__class__.__name__,\n",
        "        \"Dataset\": dataset_name,\n",
        "        \"Size (MB)\": size_mb,\n",
        "        \"MACs (M)\": macs_million,\n",
        "        \"Peak Mem (MB)\": peak_mem_mb,\n",
        "        \"Avg Mem (MB)\": avg_mem_mb,\n",
        "        \"Latency (ms)\": latency_ms,\n",
        "        \"Energy (mJ)\": energy_mj,\n",
        "        \"Top-1 (%)\": top1,\n",
        "        \"Top-5 (%)\": top5,\n",
        "    }\n",
        "    print(\"\\nProfiling Results:\")\n",
        "    for k, v in results.items():\n",
        "        print(f\"{k:<15}: {v}\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L7QUCdTolHVT"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
        "\n",
        "Transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "def cifar10_trainloader(batch_size=64, shuffle=True):\n",
        "    train = datasets.CIFAR10(root=\"./data\", train=True, transform=Transform, download=True)\n",
        "    return DataLoader(train, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "def ciaf10_testloader(batch_size=64, shuffle=False):\n",
        "    test = datasets.CIFAR10(root=\"./data\", train=False, transform=Transform, download=True)\n",
        "    return DataLoader(test, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "def cifar100_trainloader(batch_size=64, shuffle=True):\n",
        "    train = datasets.CIFAR100(root=\"./data\", train=True, transform=Transform, download=True)\n",
        "    return DataLoader(train, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "def ciaf100_testloader(batch_size=64, shuffle=False):\n",
        "    test = datasets.CIFAR100(root=\"./data\", train=False, transform=Transform, download=True)\n",
        "    return DataLoader(test, batch_size=batch_size, shuffle=shuffle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iX86eGESliJb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "datasets = {\n",
        "    \"cifar10\": {\n",
        "        \"trainloader\": cifar10_trainloader(batch_size=512),\n",
        "        \"testloader\": ciaf10_testloader(batch_size=512)\n",
        "    },\n",
        "    \"cifar100\": {\n",
        "        \"trainloader\": cifar100_trainloader(batch_size=512),\n",
        "        \"testloader\": ciaf100_testloader(batch_size=512)\n",
        "    }\n",
        "}\n",
        "testloader = datasets[\"cifar10\"][\"testloader\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8TxTOyqk0dD",
        "outputId": "49981158-2036-40cd-ef41-3f71a80f8c40"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Fatim_Sproj/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n"
          ]
        }
      ],
      "source": [
        "model10 = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_vgg16_bn\", pretrained=True)\n",
        "model100 = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar100_vgg16_bn\", pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX366hGClNtr",
        "outputId": "3af3f649-5b6b-4459-9e6e-72380705e1ab"
      },
      "outputs": [],
      "source": [
        "def load_data_and_model(dataset_name, datasets):\n",
        "    print(f\"\\Loading dataset and model for {dataset_name}\\n\")\n",
        "\n",
        "    trainloader = datasets[dataset_name][\"trainloader\"]\n",
        "    testloader = datasets[dataset_name][\"testloader\"]\n",
        "    return trainloader, testloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz-0vqsGlUKr",
        "outputId": "3eee2130-9da1-48eb-ae95-aff7ef0103fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\Loading dataset and model for cifar10\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset_name = \"cifar10\"\n",
        "trainloader10, testloader10 = load_data_and_model(dataset_name, datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKoN1Hlfmank"
      },
      "source": [
        "### Cifar 10base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z08sThOZk9mr",
        "outputId": "48709f8e-fa6e-4cf8-fa97-8c6b54e48e3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Profiling VGG on cifar10...\n",
            "\n",
            "Profiling Results:\n",
            "Model          : VGG\n",
            "Dataset        : cifar10\n",
            "Size (MB)      : 58.251\n",
            "MACs (M)       : 314.002\n",
            "Peak Mem (MB)  : 457.388\n",
            "Avg Mem (MB)   : 73.249\n",
            "Latency (ms)   : 15.461\n",
            "Energy (mJ)    : 77151.0\n",
            "Top-1 (%)      : 82.92\n",
            "Top-5 (%)      : 98.27\n"
          ]
        }
      ],
      "source": [
        "profile_results = profile_model(model10, testloader, dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auXYHGxinqeU"
      },
      "source": [
        "### cifar100 base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-YJT87ttFWK",
        "outputId": "06f58e4a-64bf-4883-aba9-22ea4dd2db9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\Loading dataset and model for cifar100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset_name = \"cifar100\"\n",
        "trainloader100, testloader100 = load_data_and_model(dataset_name, datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jVPxSzinsVj",
        "outputId": "61b60e79-320d-4034-9647-0cb4df72da6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Profiling VGG on cifar100...\n",
            "\n",
            "Profiling Results:\n",
            "Model          : VGG\n",
            "Dataset        : cifar100\n",
            "Size (MB)      : 58.427\n",
            "MACs (M)       : 314.049\n",
            "Peak Mem (MB)  : 457.722\n",
            "Avg Mem (MB)   : 73.601\n",
            "Latency (ms)   : 15.079\n",
            "Energy (mJ)    : 75541.0\n",
            "Top-1 (%)      : 63.57\n",
            "Top-5 (%)      : 83.71\n"
          ]
        }
      ],
      "source": [
        "profile_results = profile_model(model100, testloader100, dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "\n",
        "def create_sparse_weights_from_finetuned_model(finetuned_model, mask_path, output_sparse_path):\n",
        "    print(f\"Creating updated sparse weights from fine-tuned model...\")\n",
        "    masks = torch.load(mask_path, map_location='cpu', weights_only=True)\n",
        "    sparse_weights = {}\n",
        "    \n",
        "    for name, param in finetuned_model.named_parameters():\n",
        "        if name in masks:\n",
        "            mask = masks[name].to(param.device)\n",
        "            W = param.data * mask\n",
        "\n",
        "            if W.dim() == 2:\n",
        "                sparse_weights[name] = W.to_sparse_csr().cpu()\n",
        "            else: \n",
        "                original_shape = W.shape\n",
        "                W_flat = W.reshape(original_shape[0], -1)\n",
        "                sparse_csr_tensor = W_flat.to_sparse_csr().cpu()\n",
        "                sparse_weights[name] = (sparse_csr_tensor, original_shape)\n",
        "\n",
        "    torch.save(sparse_weights, output_sparse_path)\n",
        "    print(f\"Saved updated fine-tuned sparse weights to: {output_sparse_path}\")\n",
        "\n",
        "class SparseVGG(nn.Module):\n",
        "    def __init__(self, vgg_model, sparse_weights_path, device='cpu'):\n",
        "        super(SparseVGG, self).__init__()\n",
        "        self.features = vgg_model.features\n",
        "        self.classifier = vgg_model.classifier\n",
        "\n",
        "        sparse_weights = torch.load(sparse_weights_path, map_location=device, weights_only=True)\n",
        "        self._inject_sparse_weights(sparse_weights, device)\n",
        "\n",
        "    def _reconstruct_dense_conv_weight(self, sparse_object):\n",
        "        sparse_csr, original_shape = sparse_object\n",
        "        return sparse_csr.to_dense().reshape(original_shape)\n",
        "\n",
        "    def _inject_sparse_weights(self, sparse_weights, device):\n",
        "        for name, module in self.named_modules():\n",
        "            weight_name = f\"{name}.weight\"\n",
        "            if weight_name in sparse_weights:\n",
        "                sparse_object = sparse_weights[weight_name]\n",
        "                if isinstance(module, nn.Conv2d):\n",
        "                    dense_weight = self._reconstruct_dense_conv_weight(sparse_object)\n",
        "                    module.weight.data = dense_weight.to(device)\n",
        "                elif isinstance(module, nn.Linear):\n",
        "                    module.sparse_weight = sparse_object.to(device)\n",
        "                    del module.weight\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        for module in self.classifier:\n",
        "            if isinstance(module, nn.Linear) and hasattr(module, 'sparse_weight'):\n",
        "                x = torch.sparse.mm(x, module.sparse_weight.t()) + module.bias\n",
        "            else:\n",
        "                x = module(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cifar10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base model and fine-tuned state dictionary...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Fatim_Sproj/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating MACs on the original dense model...\n",
            "Creating updated sparse weights from fine-tuned model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_21336\\640391000.py:20: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\SparseCsrTensorImpl.cpp:55.)\n",
            "  sparse_csr_tensor = W_flat.to_sparse_csr().cpu()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved updated fine-tuned sparse weights to: C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\results\\sparse_weights\\cifar10_sparse_weights_finetuned.pt\n",
            "\n",
            "Instantiating custom SparseVGG model for profiling...\n",
            "Loading data...\n",
            "\n",
            "Profiling SparseVGG on cifar10...\n",
            "\n",
            "Profiling Results:\n",
            "Model          : SparseVGG\n",
            "Dataset        : cifar10\n",
            "Size (MB)      : 34.331\n",
            "MACs (M)       : 314.002\n",
            "Peak Mem (MB)  : 455.333\n",
            "Avg Mem (MB)   : 71.194\n",
            "Latency (ms)   : 17.096\n",
            "Energy (mJ)    : 78365.0\n",
            "Top-1 (%)      : 92.37\n",
            "Top-5 (%)      : 98.7\n",
            "Peak GPU memory usage: 455.373 MB\n",
            "\n",
            "--- Final Profiling Summary Table ---\n",
            "    Model Dataset  Size (MB)  MACs (M)  Peak Mem (MB)  Avg Mem (MB)  Latency (ms)  Energy (mJ)  Top-1 (%)  Top-5 (%)\n",
            "SparseVGG cifar10     34.331   314.002        455.333        71.194        17.096      78365.0      92.37       98.7\n",
            "-----------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "206"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "import multiprocessing as mp\n",
        "mp.set_start_method(\"spawn\", force=True)\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DATASET_NAME = \"cifar10\"\n",
        "\n",
        "fine_tuned_model_path = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\results\\models\\cifar10_vgg16_unstructured_finetuned_80.pt\"\n",
        "mask_path = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\results\\pruning_masks\\cifar10_unstructured_mask.pt\"\n",
        "finetuned_sparse_weights_path = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\results\\sparse_weights\\cifar10_sparse_weights_finetuned.pt\"\n",
        "\n",
        "if DEVICE == 'cuda':\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "print(\"Loading base model and fine-tuned state dictionary...\")\n",
        "base_model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_vgg16_bn\", pretrained=False)\n",
        "base_model.load_state_dict(torch.load(fine_tuned_model_path, map_location='cpu', weights_only=True))\n",
        "base_model.to(DEVICE).eval()\n",
        "\n",
        "print(\"Calculating MACs on the original dense model...\")\n",
        "macs_million = get_macs(base_model, batch_size=1, device=DEVICE, dataset_name=DATASET_NAME)\n",
        "\n",
        "create_sparse_weights_from_finetuned_model(base_model, mask_path, finetuned_sparse_weights_path)\n",
        "\n",
        "print(\"\\nInstantiating custom SparseVGG model for profiling...\")\n",
        "sparse_vgg_model = SparseVGG(base_model, finetuned_sparse_weights_path, device=DEVICE)\n",
        "sparse_vgg_model.eval()\n",
        "\n",
        "print(\"Loading data...\")\n",
        "test_loader = testloader10 \n",
        "\n",
        "if DEVICE == 'cuda':\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "profiling_results = profile_model(\n",
        "    model=sparse_vgg_model,\n",
        "    dataloader=test_loader,\n",
        "    dataset_name=DATASET_NAME,\n",
        "    device=DEVICE,\n",
        "    num_batches=10,\n",
        "    model_path_for_size=finetuned_sparse_weights_path, \n",
        "    precomputed_macs_m=macs_million\n",
        ")\n",
        "\n",
        "if DEVICE == 'cuda':\n",
        "    torch.cuda.synchronize()\n",
        "    peak_mem = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
        "    print(f\"Peak GPU memory usage: {peak_mem:.3f} MB\")\n",
        "\n",
        "print(\"\\n--- Final Profiling Summary Table ---\")\n",
        "df = pd.DataFrame([profiling_results])\n",
        "print(df.to_string(index=False))\n",
        "print(\"-----------------------------------\")\n",
        "\n",
        "del sparse_vgg_model, base_model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### cifar100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base model and fine-tuned state dictionary...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Fatim_Sproj/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating MACs on the original dense model...\n",
            "Creating updated sparse weights from fine-tuned model...\n",
            "Saved updated fine-tuned sparse weights to: C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\results\\sparse_weights\\cifar100_sparse_weights_finetuned.pt\n",
            "\n",
            "Instantiating custom SparseVGG model for profiling...\n",
            "Loading data...\n",
            "\n",
            "Profiling SparseVGG on cifar100...\n",
            "\n",
            "Profiling Results:\n",
            "Model          : SparseVGG\n",
            "Dataset        : cifar100\n",
            "Size (MB)      : 34.384\n",
            "MACs (M)       : 314.049\n",
            "Peak Mem (MB)  : 455.946\n",
            "Avg Mem (MB)   : 71.824\n",
            "Latency (ms)   : 30.656\n",
            "Energy (mJ)    : 70632.0\n",
            "Top-1 (%)      : 68.07\n",
            "Top-5 (%)      : 85.83\n",
            "Peak GPU memory usage: 456.002 MB\n",
            "\n",
            "--- Final Profiling Summary Table ---\n",
            "    Model  Dataset  Size (MB)  MACs (M)  Peak Mem (MB)  Avg Mem (MB)  Latency (ms)  Energy (mJ)  Top-1 (%)  Top-5 (%)\n",
            "SparseVGG cifar100     34.384   314.049        455.946        71.824        30.656      70632.0      68.07      85.83\n",
            "-----------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "206"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "import multiprocessing as mp\n",
        "mp.set_start_method(\"spawn\", force=True)\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DATASET_NAME = \"cifar100\"\n",
        "\n",
        "fine_tuned_model_path = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\results\\models\\cifar100_vgg16_unstructured_finetuned_80.pt\"\n",
        "mask_path = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\results\\pruning_masks\\cifar100_unstructured_mask.pt\"\n",
        "finetuned_sparse_weights_path = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\results\\sparse_weights\\cifar100_sparse_weights_finetuned.pt\"\n",
        "\n",
        "if DEVICE == 'cuda':\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "print(\"Loading base model and fine-tuned state dictionary...\")\n",
        "base_model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar100_vgg16_bn\", pretrained=False)\n",
        "base_model.load_state_dict(torch.load(fine_tuned_model_path, map_location='cpu', weights_only=True))\n",
        "base_model.to(DEVICE).eval()\n",
        "\n",
        "print(\"Calculating MACs on the original dense model...\")\n",
        "macs_million = get_macs(base_model, batch_size=1, device=DEVICE, dataset_name=DATASET_NAME)\n",
        "\n",
        "create_sparse_weights_from_finetuned_model(base_model, mask_path, finetuned_sparse_weights_path)\n",
        "\n",
        "print(\"\\nInstantiating custom SparseVGG model for profiling...\")\n",
        "sparse_vgg_model = SparseVGG(base_model, finetuned_sparse_weights_path, device=DEVICE)\n",
        "sparse_vgg_model.eval()\n",
        "\n",
        "print(\"Loading data...\")\n",
        "test_loader = testloader100 \n",
        "\n",
        "if DEVICE == 'cuda':\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "profiling_results = profile_model(\n",
        "    model=sparse_vgg_model,\n",
        "    dataloader=test_loader,\n",
        "    dataset_name=DATASET_NAME,\n",
        "    device=DEVICE,\n",
        "    num_batches=10,\n",
        "    model_path_for_size=finetuned_sparse_weights_path, \n",
        "    precomputed_macs_m=macs_million\n",
        ")\n",
        "\n",
        "if DEVICE == 'cuda':\n",
        "    torch.cuda.synchronize()\n",
        "    peak_mem = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
        "    print(f\"Peak GPU memory usage: {peak_mem:.3f} MB\")\n",
        "\n",
        "print(\"\\n--- Final Profiling Summary Table ---\")\n",
        "df = pd.DataFrame([profiling_results])\n",
        "print(df.to_string(index=False))\n",
        "print(\"-----------------------------------\")\n",
        "\n",
        "del sparse_vgg_model, base_model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base model and fine-tuned state dictionary...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Fatim_Sproj/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating MACs on the original dense model...\n",
            "Creating updated sparse weights from fine-tuned model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_21960\\640391000.py:20: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\SparseCsrTensorImpl.cpp:55.)\n",
            "  sparse_csr_tensor = W_flat.to_sparse_csr().cpu()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved updated fine-tuned sparse weights to: C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\intermediate_models\\finetuned_sparse10.pt\n",
            "\n",
            "Instantiating custom SparseVGG model for profiling...\n",
            "Loading data...\n",
            "\n",
            "Profiling SparseVGG on cifar10...\n",
            "\n",
            "Profiling Results:\n",
            "Model          : SparseVGG\n",
            "Dataset        : cifar10\n",
            "Size (MB)      : 35.105\n",
            "MACs (M)       : 314.002\n",
            "Peak Mem (MB)  : 455.873\n",
            "Avg Mem (MB)   : 71.733\n",
            "Latency (ms)   : 39.59\n",
            "Energy (mJ)    : 72881.0\n",
            "Top-1 (%)      : 82.56\n",
            "Top-5 (%)      : 97.86\n",
            "Peak GPU memory usage: 455.912 MB\n",
            "\n",
            "--- Final Profiling Summary Table ---\n",
            "    Model Dataset  Size (MB)  MACs (M)  Peak Mem (MB)  Avg Mem (MB)  Latency (ms)  Energy (mJ)  Top-1 (%)  Top-5 (%)\n",
            "SparseVGG cifar10     35.105   314.002        455.873        71.733         39.59      72881.0      82.56      97.86\n",
            "-----------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "127"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "import multiprocessing as mp\n",
        "mp.set_start_method(\"spawn\", force=True)\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DATASET_NAME = \"cifar10\"\n",
        "\n",
        "fine_tuned_model_path = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\intermediate_models\\finetuned_model10.pt\"\n",
        "mask_path = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\intermediate_models\\stage3_mask.pt\"\n",
        "finetuned_sparse_weights_path = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\intermediate_models\\finetuned_sparse10.pt\"\n",
        "\n",
        "if DEVICE == 'cuda':\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "print(\"Loading base model and fine-tuned state dictionary...\")\n",
        "base_model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_vgg16_bn\", pretrained=False)\n",
        "base_model.load_state_dict(torch.load(fine_tuned_model_path, map_location='cpu', weights_only=True))\n",
        "base_model.to(DEVICE).eval()\n",
        "\n",
        "print(\"Calculating MACs on the original dense model...\")\n",
        "macs_million = get_macs(base_model, batch_size=1, device=DEVICE, dataset_name=DATASET_NAME)\n",
        "\n",
        "create_sparse_weights_from_finetuned_model(base_model, mask_path, finetuned_sparse_weights_path)\n",
        "\n",
        "print(\"\\nInstantiating custom SparseVGG model for profiling...\")\n",
        "sparse_vgg_model = SparseVGG(base_model, finetuned_sparse_weights_path, device=DEVICE)\n",
        "sparse_vgg_model.eval()\n",
        "\n",
        "print(\"Loading data...\")\n",
        "test_loader = testloader10 \n",
        "\n",
        "if DEVICE == 'cuda':\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "profiling_results = profile_model(\n",
        "    model=sparse_vgg_model,\n",
        "    dataloader=test_loader,\n",
        "    dataset_name=DATASET_NAME,\n",
        "    device=DEVICE,\n",
        "    num_batches=10,\n",
        "    model_path_for_size=finetuned_sparse_weights_path, \n",
        "    precomputed_macs_m=macs_million\n",
        ")\n",
        "\n",
        "if DEVICE == 'cuda':\n",
        "    torch.cuda.synchronize()\n",
        "    peak_mem = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
        "    print(f\"Peak GPU memory usage: {peak_mem:.3f} MB\")\n",
        "\n",
        "print(\"\\n--- Final Profiling Summary Table ---\")\n",
        "df = pd.DataFrame([profiling_results])\n",
        "print(df.to_string(index=False))\n",
        "print(\"-----------------------------------\")\n",
        "\n",
        "del sparse_vgg_model, base_model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base model and fine-tuned state dictionary...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Fatim_Sproj/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating MACs on the original dense model...\n",
            "Creating updated sparse weights from fine-tuned model...\n",
            "Saved updated fine-tuned sparse weights to: C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\intermediate_models\\finetuned_sparse100.pt\n",
            "\n",
            "Instantiating custom SparseVGG model for profiling...\n",
            "Loading data...\n",
            "\n",
            "Profiling SparseVGG on cifar100...\n",
            "\n",
            "Profiling Results:\n",
            "Model          : SparseVGG\n",
            "Dataset        : cifar100\n",
            "Size (MB)      : 35.214\n",
            "MACs (M)       : 314.049\n",
            "Peak Mem (MB)  : 456.007\n",
            "Avg Mem (MB)   : 71.885\n",
            "Latency (ms)   : 32.875\n",
            "Energy (mJ)    : 73002.0\n",
            "Top-1 (%)      : 42.67\n",
            "Top-5 (%)      : 61.07\n",
            "Peak GPU memory usage: 456.063 MB\n",
            "\n",
            "--- Final Profiling Summary Table ---\n",
            "    Model  Dataset  Size (MB)  MACs (M)  Peak Mem (MB)  Avg Mem (MB)  Latency (ms)  Energy (mJ)  Top-1 (%)  Top-5 (%)\n",
            "SparseVGG cifar100     35.214   314.049        456.007        71.885        32.875      73002.0      42.67      61.07\n",
            "-----------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "127"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "import multiprocessing as mp\n",
        "mp.set_start_method(\"spawn\", force=True)\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DATASET_NAME = \"cifar100\"\n",
        "\n",
        "fine_tuned_model_path = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\intermediate_models\\finetuned_model100.pt\"\n",
        "mask_path = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\intermediate_models\\stage3_mask100.pt\"\n",
        "finetuned_sparse_weights_path = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\intermediate_models\\finetuned_sparse100.pt\"\n",
        "\n",
        "if DEVICE == 'cuda':\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "print(\"Loading base model and fine-tuned state dictionary...\")\n",
        "base_model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar100_vgg16_bn\", pretrained=False)\n",
        "base_model.load_state_dict(torch.load(fine_tuned_model_path, map_location='cpu', weights_only=True))\n",
        "base_model.to(DEVICE).eval()\n",
        "\n",
        "print(\"Calculating MACs on the original dense model...\")\n",
        "macs_million = get_macs(base_model, batch_size=1, device=DEVICE, dataset_name=DATASET_NAME)\n",
        "\n",
        "create_sparse_weights_from_finetuned_model(base_model, mask_path, finetuned_sparse_weights_path)\n",
        "\n",
        "print(\"\\nInstantiating custom SparseVGG model for profiling...\")\n",
        "sparse_vgg_model = SparseVGG(base_model, finetuned_sparse_weights_path, device=DEVICE)\n",
        "sparse_vgg_model.eval()\n",
        "\n",
        "print(\"Loading data...\")\n",
        "test_loader = testloader100\n",
        "\n",
        "if DEVICE == 'cuda':\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "profiling_results = profile_model(\n",
        "    model=sparse_vgg_model,\n",
        "    dataloader=test_loader,\n",
        "    dataset_name=DATASET_NAME,\n",
        "    device=DEVICE,\n",
        "    num_batches=10,\n",
        "    model_path_for_size=finetuned_sparse_weights_path, \n",
        "    precomputed_macs_m=macs_million\n",
        ")\n",
        "\n",
        "if DEVICE == 'cuda':\n",
        "    torch.cuda.synchronize()\n",
        "    peak_mem = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
        "    print(f\"Peak GPU memory usage: {peak_mem:.3f} MB\")\n",
        "\n",
        "print(\"\\n--- Final Profiling Summary Table ---\")\n",
        "df = pd.DataFrame([profiling_results])\n",
        "print(df.to_string(index=False))\n",
        "print(\"-----------------------------------\")\n",
        "\n",
        "del sparse_vgg_model, base_model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Structured Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_21312\\2424574405.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_model = torch.load(save_path_full)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (7): Conv2d(22, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (14): Conv2d(45, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (16): ReLU(inplace=True)\n",
              "    (17): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (21): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(90, 181, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): BatchNorm2d(181, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): Conv2d(181, 181, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (28): BatchNorm2d(181, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(181, 181, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): BatchNorm2d(181, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (32): ReLU(inplace=True)\n",
              "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (34): Conv2d(181, 181, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): BatchNorm2d(181, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (36): ReLU(inplace=True)\n",
              "    (37): Conv2d(181, 181, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (38): BatchNorm2d(181, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (39): ReLU(inplace=True)\n",
              "    (40): Conv2d(181, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (42): ReLU(inplace=True)\n",
              "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "save_path_full = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\cifar10_vgg16_pruned_80sparsity.pt\"\n",
        "device = \"cuda\"\n",
        "loaded_model = torch.load(save_path_full)\n",
        "loaded_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Profiling VGG on cifar10...\n",
            "\n",
            "Profiling Results:\n",
            "Model          : VGG\n",
            "Dataset        : cifar10\n",
            "Size (MB)      : 11.144\n",
            "MACs (M)       : 41.782\n",
            "Peak Mem (MB)  : 118.364\n",
            "Avg Mem (MB)   : 25.287\n",
            "Latency (ms)   : 8.342\n",
            "Energy (mJ)    : 161371.0\n",
            "Top-1 (%)      : 88.01\n",
            "Top-5 (%)      : 99.11\n"
          ]
        }
      ],
      "source": [
        "profile_results = profile_model(loaded_model, testloader10, dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_21168\\2493528711.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_model = torch.load(save_path_full)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (7): Conv2d(22, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (14): Conv2d(45, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (16): ReLU(inplace=True)\n",
              "    (17): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (21): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(90, 181, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): BatchNorm2d(181, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): Conv2d(181, 181, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (28): BatchNorm2d(181, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(181, 181, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): BatchNorm2d(181, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (32): ReLU(inplace=True)\n",
              "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (34): Conv2d(181, 181, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): BatchNorm2d(181, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (36): ReLU(inplace=True)\n",
              "    (37): Conv2d(181, 181, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (38): BatchNorm2d(181, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (39): ReLU(inplace=True)\n",
              "    (40): Conv2d(181, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (42): ReLU(inplace=True)\n",
              "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=512, out_features=100, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "save_path_full = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\cifar100_vgg16_pruned_80sparsity.pt\"\n",
        "device = \"cuda\"\n",
        "loaded_model = torch.load(save_path_full)\n",
        "loaded_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Profiling VGG on cifar100...\n",
            "\n",
            "Profiling Results:\n",
            "Model          : VGG\n",
            "Dataset        : cifar100\n",
            "Size (MB)      : 11.321\n",
            "MACs (M)       : 41.828\n",
            "Peak Mem (MB)  : 118.698\n",
            "Avg Mem (MB)   : 25.638\n",
            "Latency (ms)   : 11.238\n",
            "Energy (mJ)    : 159944.0\n",
            "Top-1 (%)      : 54.46\n",
            "Top-5 (%)      : 79.61\n"
          ]
        }
      ],
      "source": [
        "profile_results = profile_model(loaded_model, testloader100, dataset_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bacp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
