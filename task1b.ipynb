{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1396d996",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a135757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fatim_Sproj\\anaconda3\\envs\\bacp\\lib\\site-packages\\torch\\cuda\\__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from data.dataset import (\n",
    "    cifar10_trainloader,\n",
    "    ciaf10_testloader,\n",
    "    cifar100_trainloader,\n",
    "    ciaf100_testloader,\n",
    ")\n",
    "\n",
    "from pruning.GraSP import saliency_scores, rank_by_saliency, apply_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c03e8f7",
   "metadata": {},
   "source": [
    "### Model and Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2dfc304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Fatim_Sproj/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n",
      "Using cache found in C:\\Users\\Fatim_Sproj/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_vgg16_bn\", pretrained=False)\n",
    "model100 = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar100_vgg16_bn\", pretrained=False)\n",
    "CEloss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecb66332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train10 = cifar10_trainloader()\n",
    "test10= ciaf10_testloader()\n",
    "train100 = cifar100_trainloader(batch_size=256)\n",
    "test100 =  ciaf100_testloader(batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666e981",
   "metadata": {},
   "source": [
    "### Training till 20 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10df4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "indices = random.sample(range(len(train10.dataset)), 500)\n",
    "subset_500 = Subset(train10.dataset, indices)\n",
    "subset_loader = torch.utils.data.DataLoader(subset_500, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dccb9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_until(model,loss_fn, target_acc, train_loader, test_loader, device,epochs=50):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for (x, y) in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for (x_val, y_val) in test_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                preds = model(x_val)\n",
    "                predicted = preds.argmax(dim=1)\n",
    "                correct += (predicted == y_val).sum().item()\n",
    "                total += y_val.size(0)\n",
    "        val_acc = 100 * correct / total\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        if val_acc >= target_acc:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (val acc = {val_acc:.2f}%)\")\n",
    "            break\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5094d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_until_masked(model, loss_fn, optimizer, target_acc, train_loader, test_loader, device, mask_dict, epochs=50):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for (x, y) in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                for name, param in model.named_parameters():\n",
    "                    if name in mask_dict:\n",
    "                        param.mul_(mask_dict[name]) \n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for (x_val, y_val) in test_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                preds = model(x_val)\n",
    "                predicted = preds.argmax(dim=1)\n",
    "                correct += (predicted == y_val).sum().item()\n",
    "                total += y_val.size(0)\n",
    "        val_acc = 100 * correct / total\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        if val_acc >= target_acc:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (val acc = {val_acc:.2f}%)\")\n",
    "            break\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56650f",
   "metadata": {},
   "source": [
    "### Grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "919bcfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stage 0] Training randomly initialized model to ~20% accuracy\n",
      "Epoch [1/50] - Loss: 2.3014 | Val Acc: 17.70%\n",
      "Epoch [2/50] - Loss: 2.2951 | Val Acc: 23.08%\n",
      "Stopping early at epoch 2 (val acc = 23.08%)\n",
      "Saved Stage 0 trained model.\n",
      "\n",
      "\n",
      "=== Pruning Stage 1 ===\n",
      "Target sparsity: 40.0%\n",
      "→ Computing saliency scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_23752\\3745423186.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(prev_stage_model, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rank_by_saliency] Pruned 6101431 / 15253578 (target 6101431). Kept 9152147 params. threshold=6.12037e-07\n",
      "→ Applied pruning mask up to 40.0% sparsity (threshold=6.120370699136402e-07).\n",
      "Epoch [1/50] - Loss: 1.5019 | Val Acc: 55.11%\n",
      "Stopping early at epoch 1 (val acc = 55.11%)\n",
      "Stage 1 complete and saved.\n",
      "\n",
      "=== Pruning Stage 2 ===\n",
      "Target sparsity: 60.0%\n",
      "→ Computing saliency scores...\n",
      "[rank_by_saliency] Pruned 9152147 / 15253578 (target 9152147). Kept 6101431 params. threshold=0.000415872\n",
      "→ Applied pruning mask up to 60.0% sparsity (threshold=0.00041587205487303436).\n",
      "Epoch [1/50] - Loss: 1.0177 | Val Acc: 70.07%\n",
      "Stopping early at epoch 1 (val acc = 70.07%)\n",
      "Stage 2 complete and saved.\n",
      "\n",
      "=== Pruning Stage 3 ===\n",
      "Target sparsity: 80.0%\n",
      "→ Computing saliency scores...\n",
      "[rank_by_saliency] Pruned 12202862 / 15253578 (target 12202862). Kept 3050716 params. threshold=0.000828134\n",
      "→ Applied pruning mask up to 80.0% sparsity (threshold=0.0008281336631625891).\n",
      "Stage 3 complete and saved.\n",
      "\n",
      "=== Final Profiling ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_23752\\3745423186.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"stage{len(stage_fractions)}_trained_model.pt\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "initial_target_acc = 20\n",
    "final_target_sparsity = 0.8\n",
    "stage_fractions = [0.5, 0.75, 1.0] \n",
    "target_accuracies = [40, 60]      \n",
    "epochs_per_stage = 50\n",
    "\n",
    "CEloss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\n[Stage 0] Training randomly initialized model to ~20% accuracy\")\n",
    "model = train_until(\n",
    "    model=model,\n",
    "    loss_fn=CEloss,\n",
    "    target_acc=initial_target_acc,\n",
    "    train_loader=train10,\n",
    "    test_loader=test10,\n",
    "    device=device,\n",
    "    epochs=epochs_per_stage\n",
    ")\n",
    "torch.save(model.state_dict(), \"stage0_trained_model.pt\")\n",
    "print(\"Saved Stage 0 trained model.\\n\")\n",
    "\n",
    "current_mask = None\n",
    "current_sparsity = 0.0\n",
    "\n",
    "for stage_idx, fraction in enumerate(stage_fractions):\n",
    "    target_sparsity = fraction * final_target_sparsity\n",
    "    print(f\"\\n=== Pruning Stage {stage_idx + 1} ===\")\n",
    "    print(f\"Target sparsity: {target_sparsity*100:.1f}%\")\n",
    "    \n",
    "    prev_stage_model = f\"stage{stage_idx}_trained_model.pt\" if stage_idx > 0 else \"stage0_trained_model.pt\"\n",
    "    model.load_state_dict(torch.load(prev_stage_model, map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "    print(\"→ Computing saliency scores...\")\n",
    "    scores = saliency_scores(model, subset_loader, device, CEloss)\n",
    "\n",
    "    mask, thresh = rank_by_saliency(\n",
    "        scores=scores,\n",
    "        current_mask=current_mask,\n",
    "        current_sparsity=current_sparsity,\n",
    "        target_sparsity=target_sparsity\n",
    "    )\n",
    "\n",
    "    if mask is not None:\n",
    "        apply_mask(model, mask)\n",
    "        print(f\"→ Applied pruning mask up to {target_sparsity*100:.1f}% sparsity (threshold={thresh}).\")\n",
    "\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, torch.nn.BatchNorm2d):\n",
    "            m.reset_running_stats()\n",
    "\n",
    "    if stage_idx < len(stage_fractions) - 1:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "        model = train_until_masked(\n",
    "            model=model,\n",
    "            loss_fn=CEloss,\n",
    "            optimizer=optimizer,\n",
    "            target_acc=target_accuracies[stage_idx],\n",
    "            train_loader=train10,\n",
    "            test_loader=test10,\n",
    "            device=device,\n",
    "            mask_dict=mask if mask is not None else current_mask,\n",
    "            epochs=epochs_per_stage\n",
    "        )\n",
    "\n",
    "    torch.save(model.state_dict(), f\"stage{stage_idx+1}_trained_model.pt\")\n",
    "    if mask is not None:\n",
    "        torch.save(mask, f\"stage{stage_idx+1}_mask.pt\")\n",
    "        current_mask = mask\n",
    "\n",
    "    current_sparsity = target_sparsity\n",
    "    print(f\"Stage {stage_idx + 1} complete and saved.\")\n",
    "\n",
    "print(\"\\n=== Final Profiling ===\")\n",
    "model.load_state_dict(torch.load(f\"stage{len(stage_fractions)}_trained_model.pt\", map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3a6658d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Fatim_Sproj/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n",
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_25668\\3464131603.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(modelpath, map_location=device))\n",
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_25668\\3464131603.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  final_mask = torch.load(maskpath, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied final pruning mask before fine-tuning.\n",
      "Epoch [1/50] - Train Loss: 0.7103 - Test Acc: 75.71%\n",
      "--> New best model saved (epoch 1, acc 75.71%) to: best_finetuned_model10.pt\n",
      "Epoch [2/50] - Train Loss: 0.5746 - Test Acc: 77.90%\n",
      "--> New best model saved (epoch 2, acc 77.90%) to: best_finetuned_model10.pt\n",
      "Epoch [3/50] - Train Loss: 0.4755 - Test Acc: 78.16%\n",
      "--> New best model saved (epoch 3, acc 78.16%) to: best_finetuned_model10.pt\n",
      "Epoch [4/50] - Train Loss: 0.3821 - Test Acc: 79.31%\n",
      "--> New best model saved (epoch 4, acc 79.31%) to: best_finetuned_model10.pt\n",
      "Epoch [5/50] - Train Loss: 0.3118 - Test Acc: 79.90%\n",
      "--> New best model saved (epoch 5, acc 79.90%) to: best_finetuned_model10.pt\n",
      "Epoch [6/50] - Train Loss: 0.2452 - Test Acc: 79.91%\n",
      "--> New best model saved (epoch 6, acc 79.91%) to: best_finetuned_model10.pt\n",
      "Epoch [7/50] - Train Loss: 0.1961 - Test Acc: 79.73%\n",
      "Epoch [8/50] - Train Loss: 0.1605 - Test Acc: 79.51%\n",
      "Epoch [9/50] - Train Loss: 0.1391 - Test Acc: 79.45%\n",
      "Epoch [10/50] - Train Loss: 0.1153 - Test Acc: 78.86%\n",
      "Epoch [11/50] - Train Loss: 0.0976 - Test Acc: 80.51%\n",
      "--> New best model saved (epoch 11, acc 80.51%) to: best_finetuned_model10.pt\n",
      "Epoch [12/50] - Train Loss: 0.0904 - Test Acc: 79.54%\n",
      "Epoch [13/50] - Train Loss: 0.0819 - Test Acc: 81.40%\n",
      "--> New best model saved (epoch 13, acc 81.40%) to: best_finetuned_model10.pt\n",
      "Epoch [14/50] - Train Loss: 0.0732 - Test Acc: 80.57%\n",
      "Epoch [15/50] - Train Loss: 0.0659 - Test Acc: 80.05%\n",
      "Epoch [16/50] - Train Loss: 0.0634 - Test Acc: 80.18%\n",
      "Epoch [17/50] - Train Loss: 0.0637 - Test Acc: 80.49%\n",
      "Epoch [18/50] - Train Loss: 0.0566 - Test Acc: 80.05%\n",
      "Epoch [19/50] - Train Loss: 0.0574 - Test Acc: 81.13%\n",
      "Epoch [20/50] - Train Loss: 0.0489 - Test Acc: 80.56%\n",
      "Epoch [21/50] - Train Loss: 0.0447 - Test Acc: 80.26%\n",
      "Epoch [22/50] - Train Loss: 0.0475 - Test Acc: 79.30%\n",
      "Epoch [23/50] - Train Loss: 0.0494 - Test Acc: 81.04%\n",
      "Epoch [24/50] - Train Loss: 0.0446 - Test Acc: 80.54%\n",
      "Epoch [25/50] - Train Loss: 0.0416 - Test Acc: 81.41%\n",
      "--> New best model saved (epoch 25, acc 81.41%) to: best_finetuned_model10.pt\n",
      "Epoch [26/50] - Train Loss: 0.0418 - Test Acc: 79.99%\n",
      "Epoch [27/50] - Train Loss: 0.0404 - Test Acc: 80.70%\n",
      "Epoch [28/50] - Train Loss: 0.0415 - Test Acc: 81.24%\n",
      "Epoch [29/50] - Train Loss: 0.0386 - Test Acc: 80.57%\n",
      "Epoch [30/50] - Train Loss: 0.0355 - Test Acc: 81.50%\n",
      "--> New best model saved (epoch 30, acc 81.50%) to: best_finetuned_model10.pt\n",
      "Epoch [31/50] - Train Loss: 0.0333 - Test Acc: 80.74%\n",
      "Epoch [32/50] - Train Loss: 0.0397 - Test Acc: 81.73%\n",
      "--> New best model saved (epoch 32, acc 81.73%) to: best_finetuned_model10.pt\n",
      "Epoch [33/50] - Train Loss: 0.0290 - Test Acc: 81.86%\n",
      "--> New best model saved (epoch 33, acc 81.86%) to: best_finetuned_model10.pt\n",
      "Epoch [34/50] - Train Loss: 0.0324 - Test Acc: 80.81%\n",
      "Epoch [35/50] - Train Loss: 0.0320 - Test Acc: 81.49%\n",
      "Epoch [36/50] - Train Loss: 0.0343 - Test Acc: 81.67%\n",
      "Epoch [37/50] - Train Loss: 0.0309 - Test Acc: 81.47%\n",
      "Epoch [38/50] - Train Loss: 0.0268 - Test Acc: 81.27%\n",
      "Epoch [39/50] - Train Loss: 0.0328 - Test Acc: 80.61%\n",
      "Epoch [40/50] - Train Loss: 0.0283 - Test Acc: 80.75%\n",
      "Epoch [41/50] - Train Loss: 0.0260 - Test Acc: 80.44%\n",
      "Epoch [42/50] - Train Loss: 0.0299 - Test Acc: 82.00%\n",
      "--> New best model saved (epoch 42, acc 82.00%) to: best_finetuned_model10.pt\n",
      "Epoch [43/50] - Train Loss: 0.0246 - Test Acc: 80.81%\n",
      "Epoch [44/50] - Train Loss: 0.0277 - Test Acc: 81.00%\n",
      "Epoch [45/50] - Train Loss: 0.0286 - Test Acc: 81.87%\n",
      "Epoch [46/50] - Train Loss: 0.0246 - Test Acc: 81.90%\n",
      "Epoch [47/50] - Train Loss: 0.0256 - Test Acc: 82.25%\n",
      "--> New best model saved (epoch 47, acc 82.25%) to: best_finetuned_model10.pt\n",
      "Epoch [48/50] - Train Loss: 0.0238 - Test Acc: 82.39%\n",
      "--> New best model saved (epoch 48, acc 82.39%) to: best_finetuned_model10.pt\n",
      "Epoch [49/50] - Train Loss: 0.0249 - Test Acc: 82.27%\n",
      "Epoch [50/50] - Train Loss: 0.0230 - Test Acc: 82.56%\n",
      "--> New best model saved (epoch 50, acc 82.56%) to: best_finetuned_model10.pt\n",
      "Finished fine-tuning. Best epoch: 50 with Test Acc: 82.56%\n",
      "Final fine-tuned model saved to: finetuned_model10.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_vgg16_bn\", pretrained=False)\n",
    "modelpath = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\intermediate_models\\stage3_trained_model.pt\"\n",
    "model.load_state_dict(torch.load(modelpath, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "try:\n",
    "    maskpath = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\intermediate_models\\stage3_mask.pt\"\n",
    "    final_mask = torch.load(maskpath, map_location=device)\n",
    "    from pruning.GraSP import apply_mask\n",
    "    apply_mask(model, final_mask)\n",
    "    print(\"Applied final pruning mask before fine-tuning.\")\n",
    "except FileNotFoundError:\n",
    "    final_mask = None\n",
    "    print(\"No final mask found. Continuing without reapplying mask.\")\n",
    "\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.reset_running_stats()\n",
    "\n",
    "CEloss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "best_ckpt_path = \"best_finetuned_model10.pt\"\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "    return acc\n",
    "\n",
    "def finetune(model, train_loader, test_loader, loss_fn, optimizer, epochs, device, mask=None, best_ckpt_path=best_ckpt_path):\n",
    "    best_acc = -1.0\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if mask is not None:\n",
    "                apply_mask(model, mask)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "        test_acc = evaluate(model, test_loader, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{epochs}] - Train Loss: {avg_loss:.4f} - Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_epoch = epoch\n",
    "\n",
    "            if mask is not None:\n",
    "                apply_mask(model, mask)\n",
    "\n",
    "            ckpt = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_acc': best_acc,\n",
    "            }\n",
    "            torch.save(ckpt, best_ckpt_path)\n",
    "            print(f\"--> New best model saved (epoch {epoch}, acc {best_acc:.2f}%) to: {best_ckpt_path}\")\n",
    "\n",
    "    print(f\"Finished fine-tuning. Best epoch: {best_epoch} with Test Acc: {best_acc:.2f}%\")\n",
    "    return best_epoch, best_acc\n",
    "\n",
    "best_epoch, best_acc = finetune(\n",
    "    model=model,\n",
    "    train_loader=train10,\n",
    "    test_loader=test10,\n",
    "    loss_fn=CEloss,\n",
    "    optimizer=optimizer,\n",
    "    epochs=50,\n",
    "    device=device,\n",
    "    mask=final_mask,\n",
    "    best_ckpt_path=best_ckpt_path\n",
    ")\n",
    "\n",
    "final_state_path = \"finetuned_model10.pt\"\n",
    "if final_mask is not None:\n",
    "    apply_mask(model, final_mask)\n",
    "torch.save(model.state_dict(), final_state_path)\n",
    "print(f\"Final fine-tuned model saved to: {final_state_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a20214",
   "metadata": {},
   "source": [
    "### cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0912210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "indices = random.sample(range(len(train100.dataset)), 500)\n",
    "subset_500 = Subset(train10.dataset, indices)\n",
    "subset_loader = torch.utils.data.DataLoader(subset_500, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef759e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_until(model, loss_fn, optimizer, target_acc, train_loader, test_loader, device, epochs=50):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for (x, y) in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for (x_val, y_val) in test_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                preds = model(x_val)\n",
    "                predicted = preds.argmax(dim=1)\n",
    "                correct += (predicted == y_val).sum().item()\n",
    "                total += y_val.size(0)\n",
    "        \n",
    "        val_acc = 100 * correct / total\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        if val_acc >= target_acc:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (val acc = {val_acc:.2f}%)\")\n",
    "            break\n",
    "    return model\n",
    "\n",
    "def train_until_masked(model, loss_fn, optimizer, target_acc, train_loader, test_loader, device, mask_dict, epochs=50):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for (x, y) in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                for name, param in model.named_parameters():\n",
    "                    if name in mask_dict:\n",
    "                        param.mul_(mask_dict[name]) \n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for (x_val, y_val) in test_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                preds = model(x_val)\n",
    "                predicted = preds.argmax(dim=1)\n",
    "                correct += (predicted == y_val).sum().item()\n",
    "                total += y_val.size(0)\n",
    "        \n",
    "        val_acc = 100 * correct / total\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        if val_acc >= target_acc:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (val acc = {val_acc:.2f}%)\")\n",
    "            break\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d77e682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stage 0] Training CIFAR-100 model to ~20% accuracy\n",
      "Epoch [1/50] - Loss: 4.3059 | Val Acc: 3.13%\n",
      "Epoch [2/50] - Loss: 4.0419 | Val Acc: 5.96%\n",
      "Epoch [3/50] - Loss: 3.8464 | Val Acc: 8.51%\n",
      "Epoch [4/50] - Loss: 3.5799 | Val Acc: 11.58%\n",
      "Epoch [5/50] - Loss: 3.2819 | Val Acc: 18.06%\n",
      "Epoch [6/50] - Loss: 3.0223 | Val Acc: 21.48%\n",
      "Stopping early at epoch 6 (val acc = 21.48%)\n",
      "Saved Stage 0 trained model for CIFAR-100.\n",
      "\n",
      "\n",
      "=== Pruning Stage 1 ===\n",
      "Target sparsity: 40.0%\n",
      "→ Computing saliency scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_30380\\1526642278.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model100.load_state_dict(torch.load(prev_stage_model, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rank_by_saliency] Pruned 6119899 / 15299748 (target 6119899). Kept 9179849 params. threshold=0.000217016\n",
      "→ Applied pruning mask up to 40.0% sparsity (threshold=0.00021701646619476378).\n",
      "Epoch [1/50] - Loss: 2.6827 | Val Acc: 27.75%\n",
      "Epoch [2/50] - Loss: 2.5425 | Val Acc: 29.68%\n",
      "Epoch [3/50] - Loss: 2.4388 | Val Acc: 31.33%\n",
      "Epoch [4/50] - Loss: 2.3319 | Val Acc: 32.72%\n",
      "Epoch [5/50] - Loss: 2.2289 | Val Acc: 34.07%\n",
      "Epoch [6/50] - Loss: 2.1276 | Val Acc: 34.49%\n",
      "Epoch [7/50] - Loss: 2.0274 | Val Acc: 35.49%\n",
      "Epoch [8/50] - Loss: 1.9306 | Val Acc: 36.35%\n",
      "Epoch [9/50] - Loss: 1.8410 | Val Acc: 36.67%\n",
      "Epoch [10/50] - Loss: 1.7530 | Val Acc: 37.42%\n",
      "Epoch [11/50] - Loss: 1.6629 | Val Acc: 37.33%\n",
      "Epoch [12/50] - Loss: 1.5765 | Val Acc: 38.21%\n",
      "Epoch [13/50] - Loss: 1.4927 | Val Acc: 38.02%\n",
      "Epoch [14/50] - Loss: 1.4070 | Val Acc: 38.09%\n",
      "Epoch [15/50] - Loss: 1.3287 | Val Acc: 38.03%\n",
      "Epoch [16/50] - Loss: 1.2641 | Val Acc: 37.63%\n",
      "Epoch [17/50] - Loss: 1.1885 | Val Acc: 38.58%\n",
      "Epoch [18/50] - Loss: 1.1218 | Val Acc: 39.07%\n",
      "Epoch [19/50] - Loss: 1.0582 | Val Acc: 39.21%\n",
      "Epoch [20/50] - Loss: 1.0070 | Val Acc: 38.69%\n",
      "Epoch [21/50] - Loss: 0.9471 | Val Acc: 38.48%\n",
      "Epoch [22/50] - Loss: 0.9034 | Val Acc: 37.85%\n",
      "Epoch [23/50] - Loss: 0.8481 | Val Acc: 39.16%\n",
      "Epoch [24/50] - Loss: 0.8204 | Val Acc: 39.25%\n",
      "Epoch [25/50] - Loss: 0.7668 | Val Acc: 38.21%\n",
      "Epoch [26/50] - Loss: 0.7467 | Val Acc: 38.95%\n",
      "Epoch [27/50] - Loss: 0.7116 | Val Acc: 39.22%\n",
      "Epoch [28/50] - Loss: 0.6642 | Val Acc: 38.20%\n",
      "Epoch [29/50] - Loss: 0.6347 | Val Acc: 38.81%\n",
      "Epoch [30/50] - Loss: 0.6125 | Val Acc: 39.03%\n",
      "Epoch [31/50] - Loss: 0.5776 | Val Acc: 39.12%\n",
      "Epoch [32/50] - Loss: 0.5674 | Val Acc: 39.13%\n",
      "Epoch [33/50] - Loss: 0.5330 | Val Acc: 39.18%\n",
      "Epoch [34/50] - Loss: 0.5168 | Val Acc: 39.33%\n",
      "Epoch [35/50] - Loss: 0.5035 | Val Acc: 38.96%\n",
      "Epoch [36/50] - Loss: 0.4741 | Val Acc: 39.28%\n",
      "Epoch [37/50] - Loss: 0.4528 | Val Acc: 39.80%\n",
      "Epoch [38/50] - Loss: 0.4454 | Val Acc: 39.89%\n",
      "Epoch [39/50] - Loss: 0.4297 | Val Acc: 37.98%\n",
      "Epoch [40/50] - Loss: 0.4049 | Val Acc: 39.41%\n",
      "Epoch [41/50] - Loss: 0.3903 | Val Acc: 39.05%\n",
      "Epoch [42/50] - Loss: 0.3802 | Val Acc: 40.43%\n",
      "Stopping early at epoch 42 (val acc = 40.43%)\n",
      "Stage 1 complete and saved.\n",
      "\n",
      "=== Pruning Stage 2 ===\n",
      "Target sparsity: 60.0%\n",
      "→ Computing saliency scores...\n",
      "[rank_by_saliency] Pruned 9179849 / 15299748 (target 9179849). Kept 6119899 params. threshold=0.142452\n",
      "→ Applied pruning mask up to 60.0% sparsity (threshold=0.14245177805423737).\n",
      "Epoch [1/50] - Loss: 0.7244 | Val Acc: 38.75%\n",
      "Epoch [2/50] - Loss: 0.5541 | Val Acc: 38.48%\n",
      "Epoch [3/50] - Loss: 0.5241 | Val Acc: 39.04%\n",
      "Epoch [4/50] - Loss: 0.4876 | Val Acc: 39.66%\n",
      "Epoch [5/50] - Loss: 0.4609 | Val Acc: 39.47%\n",
      "Epoch [6/50] - Loss: 0.4515 | Val Acc: 39.24%\n",
      "Epoch [7/50] - Loss: 0.4199 | Val Acc: 39.28%\n",
      "Epoch [8/50] - Loss: 0.4156 | Val Acc: 39.10%\n",
      "Epoch [9/50] - Loss: 0.3989 | Val Acc: 40.08%\n",
      "Epoch [10/50] - Loss: 0.3786 | Val Acc: 39.22%\n",
      "Epoch [11/50] - Loss: 0.3749 | Val Acc: 39.50%\n",
      "Epoch [12/50] - Loss: 0.3780 | Val Acc: 39.87%\n",
      "Epoch [13/50] - Loss: 0.3700 | Val Acc: 39.32%\n",
      "Epoch [14/50] - Loss: 0.3508 | Val Acc: 39.41%\n",
      "Epoch [15/50] - Loss: 0.3447 | Val Acc: 39.93%\n",
      "Epoch [16/50] - Loss: 0.3400 | Val Acc: 39.94%\n",
      "Epoch [17/50] - Loss: 0.3238 | Val Acc: 39.83%\n",
      "Epoch [18/50] - Loss: 0.3317 | Val Acc: 39.61%\n",
      "Epoch [19/50] - Loss: 0.3309 | Val Acc: 39.86%\n",
      "Epoch [20/50] - Loss: 0.3137 | Val Acc: 40.47%\n",
      "Epoch [21/50] - Loss: 0.3077 | Val Acc: 39.68%\n",
      "Epoch [22/50] - Loss: 0.3030 | Val Acc: 40.77%\n",
      "Epoch [23/50] - Loss: 0.3026 | Val Acc: 40.89%\n",
      "Epoch [24/50] - Loss: 0.2908 | Val Acc: 40.04%\n",
      "Epoch [25/50] - Loss: 0.2883 | Val Acc: 40.29%\n",
      "Epoch [26/50] - Loss: 0.2802 | Val Acc: 40.42%\n",
      "Epoch [27/50] - Loss: 0.2927 | Val Acc: 40.36%\n",
      "Epoch [28/50] - Loss: 0.2769 | Val Acc: 39.23%\n",
      "Epoch [29/50] - Loss: 0.2777 | Val Acc: 40.60%\n",
      "Epoch [30/50] - Loss: 0.2715 | Val Acc: 39.62%\n",
      "Epoch [31/50] - Loss: 0.2691 | Val Acc: 39.92%\n",
      "Epoch [32/50] - Loss: 0.2661 | Val Acc: 40.78%\n",
      "Epoch [33/50] - Loss: 0.2620 | Val Acc: 39.46%\n",
      "Epoch [34/50] - Loss: 0.2658 | Val Acc: 40.35%\n",
      "Epoch [35/50] - Loss: 0.2473 | Val Acc: 39.93%\n",
      "Epoch [36/50] - Loss: 0.2374 | Val Acc: 40.07%\n",
      "Epoch [37/50] - Loss: 0.2448 | Val Acc: 41.10%\n",
      "Epoch [38/50] - Loss: 0.2510 | Val Acc: 40.50%\n",
      "Epoch [39/50] - Loss: 0.2468 | Val Acc: 41.16%\n",
      "Epoch [40/50] - Loss: 0.2448 | Val Acc: 40.37%\n",
      "Epoch [41/50] - Loss: 0.2408 | Val Acc: 40.56%\n",
      "Epoch [42/50] - Loss: 0.2278 | Val Acc: 40.94%\n",
      "Epoch [43/50] - Loss: 0.2321 | Val Acc: 39.62%\n",
      "Epoch [44/50] - Loss: 0.2269 | Val Acc: 41.27%\n",
      "Epoch [45/50] - Loss: 0.2201 | Val Acc: 41.10%\n",
      "Epoch [46/50] - Loss: 0.2143 | Val Acc: 40.85%\n",
      "Epoch [47/50] - Loss: 0.2201 | Val Acc: 41.04%\n",
      "Epoch [48/50] - Loss: 0.2177 | Val Acc: 41.66%\n",
      "Epoch [49/50] - Loss: 0.2159 | Val Acc: 41.48%\n",
      "Epoch [50/50] - Loss: 0.2030 | Val Acc: 41.25%\n",
      "Stage 2 complete and saved.\n",
      "\n",
      "=== Pruning Stage 3 ===\n",
      "Target sparsity: 80.0%\n",
      "→ Computing saliency scores...\n",
      "[rank_by_saliency] Pruned 12239798 / 15299748 (target 12239798). Kept 3059950 params. threshold=1.00264\n",
      "→ Applied pruning mask up to 80.0% sparsity (threshold=1.0026357173919678).\n",
      "Stage 3 complete and saved.\n",
      "\n",
      "=== Final Profiling ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_30380\\1526642278.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model100.load_state_dict(torch.load(f\"stage{len(stage_fractions)}_trained_model100.pt\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model100 = model100.to(device)\n",
    "\n",
    "initial_target_acc = 20\n",
    "final_target_sparsity = 0.8\n",
    "stage_fractions = [0.5, 0.75, 1.0] \n",
    "target_accuracies = [40, 60]      \n",
    "epochs_per_stage = 50\n",
    "\n",
    "CEloss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_100 = torch.optim.Adam(model100.parameters(), lr=3e-4)\n",
    "\n",
    "print(\"\\n[Stage 0] Training CIFAR-100 model to ~20% accuracy\")\n",
    "model100 = train_until(\n",
    "    model=model100,\n",
    "    loss_fn=CEloss,\n",
    "    optimizer=optimizer_100,\n",
    "    target_acc=initial_target_acc,\n",
    "    train_loader=train100, \n",
    "    test_loader=test100,   \n",
    "    device=device,\n",
    "    epochs=epochs_per_stage\n",
    ")\n",
    "torch.save(model100.state_dict(), \"stage0_trained_model100.pt\")\n",
    "print(\"Saved Stage 0 trained model for CIFAR-100.\\n\")\n",
    "\n",
    "current_mask = None\n",
    "current_sparsity = 0.0\n",
    "\n",
    "for stage_idx, fraction in enumerate(stage_fractions):\n",
    "    target_sparsity = fraction * final_target_sparsity\n",
    "    print(f\"\\n=== Pruning Stage {stage_idx + 1} ===\")\n",
    "    print(f\"Target sparsity: {target_sparsity*100:.1f}%\")\n",
    "    \n",
    "    prev_stage_model = f\"stage{stage_idx}_trained_model100.pt\" if stage_idx > 0 else \"stage0_trained_model100.pt\"\n",
    "    model100.load_state_dict(torch.load(prev_stage_model, map_location=device))\n",
    "    model100.to(device)\n",
    "\n",
    "    print(\"→ Computing saliency scores...\")\n",
    "    scores = saliency_scores(model100, subset_loader, device, CEloss) \n",
    "\n",
    "    mask, thresh = rank_by_saliency(\n",
    "        scores=scores,\n",
    "        current_mask=current_mask,\n",
    "        current_sparsity=current_sparsity,\n",
    "        target_sparsity=target_sparsity\n",
    "    )\n",
    "\n",
    "    if mask is not None:\n",
    "        apply_mask(model100, mask)\n",
    "        print(f\"→ Applied pruning mask up to {target_sparsity*100:.1f}% sparsity (threshold={thresh}).\")\n",
    "\n",
    "    for m in model100.modules():\n",
    "        if isinstance(m, torch.nn.BatchNorm2d):\n",
    "            m.reset_running_stats()\n",
    "\n",
    "    if stage_idx < len(stage_fractions) - 1:\n",
    "        optimizer_stage = torch.optim.Adam(model100.parameters(), lr=1e-4)\n",
    "        model100 = train_until_masked(\n",
    "            model=model100,\n",
    "            loss_fn=CEloss,\n",
    "            optimizer=optimizer_stage, \n",
    "            target_acc=target_accuracies[stage_idx],\n",
    "            train_loader=train100,\n",
    "            test_loader=test100,  \n",
    "            device=device,\n",
    "            mask_dict=mask if mask is not None else current_mask,\n",
    "            epochs=epochs_per_stage\n",
    "        )\n",
    "    torch.save(model100.state_dict(), f\"stage{stage_idx+1}_trained_model100.pt\")\n",
    "    if mask is not None:\n",
    "        torch.save(mask, f\"stage{stage_idx+1}_mask100.pt\")\n",
    "        current_mask = mask\n",
    "    current_sparsity = target_sparsity\n",
    "    print(f\"Stage {stage_idx + 1} complete and saved.\")\n",
    "print(\"\\n=== Final Profiling ===\\n\")\n",
    "model100.load_state_dict(torch.load(f\"stage{len(stage_fractions)}_trained_model100.pt\", map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a05f736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Fatim_Sproj/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n",
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_30380\\3612896447.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(modelpath, map_location=device))\n",
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_30380\\3612896447.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  final_mask = torch.load(maskpath, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied final pruning mask before fine-tuning.\n",
      "Epoch [1/50] - Train Loss: 1.3987 - Test Acc: 38.68%\n",
      "--> New best model saved (epoch 1, acc 38.68%) to: best_finetuned_model100.pt\n",
      "Epoch [2/50] - Train Loss: 0.9197 - Test Acc: 40.10%\n",
      "--> New best model saved (epoch 2, acc 40.10%) to: best_finetuned_model100.pt\n",
      "Epoch [3/50] - Train Loss: 0.7861 - Test Acc: 40.69%\n",
      "--> New best model saved (epoch 3, acc 40.69%) to: best_finetuned_model100.pt\n",
      "Epoch [4/50] - Train Loss: 0.7156 - Test Acc: 40.95%\n",
      "--> New best model saved (epoch 4, acc 40.95%) to: best_finetuned_model100.pt\n",
      "Epoch [5/50] - Train Loss: 0.6628 - Test Acc: 41.05%\n",
      "--> New best model saved (epoch 5, acc 41.05%) to: best_finetuned_model100.pt\n",
      "Epoch [6/50] - Train Loss: 0.6268 - Test Acc: 41.05%\n",
      "Epoch [7/50] - Train Loss: 0.6013 - Test Acc: 41.40%\n",
      "--> New best model saved (epoch 7, acc 41.40%) to: best_finetuned_model100.pt\n",
      "Epoch [8/50] - Train Loss: 0.5744 - Test Acc: 41.30%\n",
      "Epoch [9/50] - Train Loss: 0.5607 - Test Acc: 41.56%\n",
      "--> New best model saved (epoch 9, acc 41.56%) to: best_finetuned_model100.pt\n",
      "Epoch [10/50] - Train Loss: 0.5458 - Test Acc: 41.64%\n",
      "--> New best model saved (epoch 10, acc 41.64%) to: best_finetuned_model100.pt\n",
      "Epoch [11/50] - Train Loss: 0.5299 - Test Acc: 41.19%\n",
      "Epoch [12/50] - Train Loss: 0.5225 - Test Acc: 41.59%\n",
      "Epoch [13/50] - Train Loss: 0.5116 - Test Acc: 41.42%\n",
      "Epoch [14/50] - Train Loss: 0.5061 - Test Acc: 41.75%\n",
      "--> New best model saved (epoch 14, acc 41.75%) to: best_finetuned_model100.pt\n",
      "Epoch [15/50] - Train Loss: 0.4948 - Test Acc: 41.53%\n",
      "Epoch [16/50] - Train Loss: 0.4925 - Test Acc: 41.57%\n",
      "Epoch [17/50] - Train Loss: 0.4797 - Test Acc: 41.62%\n",
      "Epoch [18/50] - Train Loss: 0.4764 - Test Acc: 41.76%\n",
      "--> New best model saved (epoch 18, acc 41.76%) to: best_finetuned_model100.pt\n",
      "Epoch [19/50] - Train Loss: 0.4715 - Test Acc: 42.01%\n",
      "--> New best model saved (epoch 19, acc 42.01%) to: best_finetuned_model100.pt\n",
      "Epoch [20/50] - Train Loss: 0.4683 - Test Acc: 41.55%\n",
      "Epoch [21/50] - Train Loss: 0.4635 - Test Acc: 41.78%\n",
      "Epoch [22/50] - Train Loss: 0.4534 - Test Acc: 41.35%\n",
      "Epoch [23/50] - Train Loss: 0.4530 - Test Acc: 41.97%\n",
      "Epoch [24/50] - Train Loss: 0.4538 - Test Acc: 41.78%\n",
      "Epoch [25/50] - Train Loss: 0.4428 - Test Acc: 41.92%\n",
      "Epoch [26/50] - Train Loss: 0.4444 - Test Acc: 41.55%\n",
      "Epoch [27/50] - Train Loss: 0.4413 - Test Acc: 41.86%\n",
      "Epoch [28/50] - Train Loss: 0.4425 - Test Acc: 42.07%\n",
      "--> New best model saved (epoch 28, acc 42.07%) to: best_finetuned_model100.pt\n",
      "Epoch [29/50] - Train Loss: 0.4372 - Test Acc: 41.28%\n",
      "Epoch [30/50] - Train Loss: 0.4346 - Test Acc: 41.73%\n",
      "Epoch [31/50] - Train Loss: 0.4342 - Test Acc: 42.09%\n",
      "--> New best model saved (epoch 31, acc 42.09%) to: best_finetuned_model100.pt\n",
      "Epoch [32/50] - Train Loss: 0.4316 - Test Acc: 41.53%\n",
      "Epoch [33/50] - Train Loss: 0.4266 - Test Acc: 42.07%\n",
      "Epoch [34/50] - Train Loss: 0.4285 - Test Acc: 41.76%\n",
      "Epoch [35/50] - Train Loss: 0.4236 - Test Acc: 42.32%\n",
      "--> New best model saved (epoch 35, acc 42.32%) to: best_finetuned_model100.pt\n",
      "Epoch [36/50] - Train Loss: 0.4217 - Test Acc: 42.12%\n",
      "Epoch [37/50] - Train Loss: 0.4226 - Test Acc: 42.02%\n",
      "Epoch [38/50] - Train Loss: 0.4214 - Test Acc: 41.94%\n",
      "Epoch [39/50] - Train Loss: 0.4180 - Test Acc: 41.52%\n",
      "Epoch [40/50] - Train Loss: 0.4144 - Test Acc: 41.72%\n",
      "Epoch [41/50] - Train Loss: 0.4122 - Test Acc: 42.00%\n",
      "Epoch [42/50] - Train Loss: 0.4032 - Test Acc: 42.23%\n",
      "Epoch [43/50] - Train Loss: 0.4069 - Test Acc: 42.36%\n",
      "--> New best model saved (epoch 43, acc 42.36%) to: best_finetuned_model100.pt\n",
      "Epoch [44/50] - Train Loss: 0.4091 - Test Acc: 41.75%\n",
      "Epoch [45/50] - Train Loss: 0.4068 - Test Acc: 42.48%\n",
      "--> New best model saved (epoch 45, acc 42.48%) to: best_finetuned_model100.pt\n",
      "Epoch [46/50] - Train Loss: 0.4050 - Test Acc: 42.81%\n",
      "--> New best model saved (epoch 46, acc 42.81%) to: best_finetuned_model100.pt\n",
      "Epoch [47/50] - Train Loss: 0.3992 - Test Acc: 42.42%\n",
      "Epoch [48/50] - Train Loss: 0.4028 - Test Acc: 42.89%\n",
      "--> New best model saved (epoch 48, acc 42.89%) to: best_finetuned_model100.pt\n",
      "Epoch [49/50] - Train Loss: 0.4011 - Test Acc: 42.62%\n",
      "Epoch [50/50] - Train Loss: 0.4013 - Test Acc: 42.69%\n",
      "Finished fine-tuning. Best epoch: 48 with Test Acc: 42.89%\n",
      "Final fine-tuned model saved to: finetuned_model100.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar100_vgg16_bn\", pretrained=False)\n",
    "modelpath = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\intermediate_models\\stage3_trained_model100.pt\"\n",
    "model.load_state_dict(torch.load(modelpath, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "try:\n",
    "    maskpath = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\intermediate_models\\stage3_mask100.pt\"\n",
    "    final_mask = torch.load(maskpath, map_location=device)\n",
    "    from pruning.GraSP import apply_mask\n",
    "    apply_mask(model, final_mask)\n",
    "    print(\"Applied final pruning mask before fine-tuning.\")\n",
    "except FileNotFoundError:\n",
    "    final_mask = None\n",
    "    print(\"No final mask found. Continuing without reapplying mask.\")\n",
    "\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.reset_running_stats()\n",
    "\n",
    "CEloss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "best_ckpt_path = \"best_finetuned_model100.pt\"\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "    return acc\n",
    "\n",
    "def finetune(model, train_loader, test_loader, loss_fn, optimizer, epochs, device, mask=None, best_ckpt_path=best_ckpt_path):\n",
    "    best_acc = -1.0\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if mask is not None:\n",
    "                apply_mask(model, mask)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "        test_acc = evaluate(model, test_loader, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{epochs}] - Train Loss: {avg_loss:.4f} - Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_epoch = epoch\n",
    "\n",
    "            if mask is not None:\n",
    "                apply_mask(model, mask)\n",
    "\n",
    "            ckpt = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_acc': best_acc,\n",
    "            }\n",
    "            torch.save(ckpt, best_ckpt_path)\n",
    "            print(f\"--> New best model saved (epoch {epoch}, acc {best_acc:.2f}%) to: {best_ckpt_path}\")\n",
    "\n",
    "    print(f\"Finished fine-tuning. Best epoch: {best_epoch} with Test Acc: {best_acc:.2f}%\")\n",
    "    return best_epoch, best_acc\n",
    "\n",
    "best_epoch, best_acc = finetune(\n",
    "    model=model,\n",
    "    train_loader=train100,\n",
    "    test_loader=test100,\n",
    "    loss_fn=CEloss,\n",
    "    optimizer=optimizer,\n",
    "    epochs=50,\n",
    "    device=device,\n",
    "    mask=final_mask,\n",
    "    best_ckpt_path=best_ckpt_path\n",
    ")\n",
    "\n",
    "final_state_path = \"finetuned_model100.pt\"\n",
    "if final_mask is not None:\n",
    "    apply_mask(model, final_mask)\n",
    "torch.save(model.state_dict(), final_state_path)\n",
    "print(f\"Final fine-tuned model saved to: {final_state_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bacp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
