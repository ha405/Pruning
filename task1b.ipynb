{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1396d996",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a135757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fatim_Sproj\\anaconda3\\envs\\bacp\\lib\\site-packages\\torch\\cuda\\__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from data.dataset import (\n",
    "    cifar10_trainloader,\n",
    "    ciaf10_testloader,\n",
    "    cifar100_trainloader,\n",
    "    ciaf100_testloader,\n",
    ")\n",
    "from pruning.GraSP import saliency_scores, rank_by_saliency, apply_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c03e8f7",
   "metadata": {},
   "source": [
    "### Model and Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2dfc304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Fatim_Sproj/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n",
      "Using cache found in C:\\Users\\Fatim_Sproj/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_vgg16_bn\", pretrained=False)\n",
    "model100 = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar100_vgg16_bn\", pretrained=False)\n",
    "CEloss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb66332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train10 = cifar10_trainloader()\n",
    "test10= ciaf10_testloader()\n",
    "train100 = cifar100_trainloader(batch_size=256)\n",
    "test100 =  ciaf100_testloader(batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666e981",
   "metadata": {},
   "source": [
    "### Training till 20 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10df4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "indices = random.sample(range(len(train10.dataset)), 500)\n",
    "subset_500 = Subset(train10.dataset, indices)\n",
    "subset_loader = torch.utils.data.DataLoader(subset_500, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dccb9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_until(model,loss_fn, target_acc, train_loader, test_loader, device,epochs=50):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for (x, y) in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for (x_val, y_val) in test_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                preds = model(x_val)\n",
    "                predicted = preds.argmax(dim=1)\n",
    "                correct += (predicted == y_val).sum().item()\n",
    "                total += y_val.size(0)\n",
    "        val_acc = 100 * correct / total\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        if val_acc >= target_acc:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (val acc = {val_acc:.2f}%)\")\n",
    "            break\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5094d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_until_masked(model, loss_fn, optimizer, target_acc, train_loader, test_loader, device, mask_dict, epochs=50):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for (x, y) in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                for name, param in model.named_parameters():\n",
    "                    if name in mask_dict:\n",
    "                        param.mul_(mask_dict[name]) \n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for (x_val, y_val) in test_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                preds = model(x_val)\n",
    "                predicted = preds.argmax(dim=1)\n",
    "                correct += (predicted == y_val).sum().item()\n",
    "                total += y_val.size(0)\n",
    "        val_acc = 100 * correct / total\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        if val_acc >= target_acc:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (val acc = {val_acc:.2f}%)\")\n",
    "            break\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56650f",
   "metadata": {},
   "source": [
    "### Grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "919bcfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stage 0] Training randomly initialized model to ~20% accuracy\n",
      "Epoch [1/50] - Loss: 2.3017 | Val Acc: 16.74%\n",
      "Epoch [2/50] - Loss: 2.2971 | Val Acc: 20.46%\n",
      "Stopping early at epoch 2 (val acc = 20.46%)\n",
      "Saved Stage 0 trained model.\n",
      "\n",
      "\n",
      "=== Pruning Stage 1 ===\n",
      "Target sparsity: 40.0%\n",
      "→ Computing saliency scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_5916\\3745423186.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(prev_stage_model, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rank_by_saliency] Pruned 6101431 / 15253578 (target 6101431). Kept 9152147 params. threshold=5.49452e-07\n",
      "→ Applied pruning mask up to 40.0% sparsity (threshold=5.494516130966076e-07).\n",
      "Epoch [1/50] - Loss: 1.6394 | Val Acc: 48.50%\n",
      "Stopping early at epoch 1 (val acc = 48.50%)\n",
      "Stage 1 complete and saved.\n",
      "\n",
      "=== Pruning Stage 2 ===\n",
      "Target sparsity: 60.0%\n",
      "→ Computing saliency scores...\n",
      "[rank_by_saliency] Pruned 9152148 / 15253578 (target 9152147). Kept 6101430 params. threshold=0.000128909\n",
      "→ Applied pruning mask up to 60.0% sparsity (threshold=0.00012890863581560552).\n",
      "Epoch [1/50] - Loss: 1.1848 | Val Acc: 65.23%\n",
      "Stopping early at epoch 1 (val acc = 65.23%)\n",
      "Stage 2 complete and saved.\n",
      "\n",
      "=== Pruning Stage 3 ===\n",
      "Target sparsity: 80.0%\n",
      "→ Computing saliency scores...\n",
      "[rank_by_saliency] Pruned 12202863 / 15253578 (target 12202862). Kept 3050715 params. threshold=0.000803289\n",
      "→ Applied pruning mask up to 80.0% sparsity (threshold=0.000803289411123842).\n",
      "Stage 3 complete and saved.\n",
      "\n",
      "=== Final Profiling ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_5916\\3745423186.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"stage{len(stage_fractions)}_trained_model.pt\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "initial_target_acc = 20\n",
    "final_target_sparsity = 0.8\n",
    "stage_fractions = [0.5, 0.75, 1.0] \n",
    "target_accuracies = [40, 60]      \n",
    "epochs_per_stage = 50\n",
    "\n",
    "CEloss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\n[Stage 0] Training randomly initialized model to ~20% accuracy\")\n",
    "model = train_until(\n",
    "    model=model,\n",
    "    loss_fn=CEloss,\n",
    "    target_acc=initial_target_acc,\n",
    "    train_loader=train10,\n",
    "    test_loader=test10,\n",
    "    device=device,\n",
    "    epochs=epochs_per_stage\n",
    ")\n",
    "torch.save(model.state_dict(), \"stage0_trained_model.pt\")\n",
    "print(\"Saved Stage 0 trained model.\\n\")\n",
    "\n",
    "current_mask = None\n",
    "current_sparsity = 0.0\n",
    "\n",
    "for stage_idx, fraction in enumerate(stage_fractions):\n",
    "    target_sparsity = fraction * final_target_sparsity\n",
    "    print(f\"\\n=== Pruning Stage {stage_idx + 1} ===\")\n",
    "    print(f\"Target sparsity: {target_sparsity*100:.1f}%\")\n",
    "    \n",
    "    prev_stage_model = f\"stage{stage_idx}_trained_model.pt\" if stage_idx > 0 else \"stage0_trained_model.pt\"\n",
    "    model.load_state_dict(torch.load(prev_stage_model, map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "    print(\"→ Computing saliency scores...\")\n",
    "    scores = saliency_scores(model, subset_loader, device, CEloss)\n",
    "\n",
    "    mask, thresh = rank_by_saliency(\n",
    "        scores=scores,\n",
    "        current_mask=current_mask,\n",
    "        current_sparsity=current_sparsity,\n",
    "        target_sparsity=target_sparsity\n",
    "    )\n",
    "\n",
    "    if mask is not None:\n",
    "        apply_mask(model, mask)\n",
    "        print(f\"→ Applied pruning mask up to {target_sparsity*100:.1f}% sparsity (threshold={thresh}).\")\n",
    "\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, torch.nn.BatchNorm2d):\n",
    "            m.reset_running_stats()\n",
    "\n",
    "    if stage_idx < len(stage_fractions) - 1:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "        model = train_until_masked(\n",
    "            model=model,\n",
    "            loss_fn=CEloss,\n",
    "            optimizer=optimizer,\n",
    "            target_acc=target_accuracies[stage_idx],\n",
    "            train_loader=train10,\n",
    "            test_loader=test10,\n",
    "            device=device,\n",
    "            mask_dict=mask if mask is not None else current_mask,\n",
    "            epochs=epochs_per_stage\n",
    "        )\n",
    "\n",
    "    torch.save(model.state_dict(), f\"stage{stage_idx+1}_trained_model.pt\")\n",
    "    if mask is not None:\n",
    "        torch.save(mask, f\"stage{stage_idx+1}_mask.pt\")\n",
    "        current_mask = mask\n",
    "\n",
    "    current_sparsity = target_sparsity\n",
    "    print(f\"Stage {stage_idx + 1} complete and saved.\")\n",
    "\n",
    "print(\"\\n=== Final Profiling ===\")\n",
    "model.load_state_dict(torch.load(f\"stage{len(stage_fractions)}_trained_model.pt\", map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3a6658d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Fatim_Sproj/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n",
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_5916\\3464131603.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(modelpath, map_location=device))\n",
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_5916\\3464131603.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  final_mask = torch.load(maskpath, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied final pruning mask before fine-tuning.\n",
      "Epoch [1/50] - Train Loss: 0.9047 - Test Acc: 72.73%\n",
      "--> New best model saved (epoch 1, acc 72.73%) to: best_finetuned_model10.pt\n",
      "Epoch [2/50] - Train Loss: 0.7952 - Test Acc: 73.83%\n",
      "--> New best model saved (epoch 2, acc 73.83%) to: best_finetuned_model10.pt\n",
      "Epoch [3/50] - Train Loss: 0.7260 - Test Acc: 76.22%\n",
      "--> New best model saved (epoch 3, acc 76.22%) to: best_finetuned_model10.pt\n",
      "Epoch [4/50] - Train Loss: 0.6764 - Test Acc: 78.53%\n",
      "--> New best model saved (epoch 4, acc 78.53%) to: best_finetuned_model10.pt\n",
      "Epoch [5/50] - Train Loss: 0.6341 - Test Acc: 78.92%\n",
      "--> New best model saved (epoch 5, acc 78.92%) to: best_finetuned_model10.pt\n",
      "Epoch [6/50] - Train Loss: 0.5963 - Test Acc: 80.01%\n",
      "--> New best model saved (epoch 6, acc 80.01%) to: best_finetuned_model10.pt\n",
      "Epoch [7/50] - Train Loss: 0.5578 - Test Acc: 80.54%\n",
      "--> New best model saved (epoch 7, acc 80.54%) to: best_finetuned_model10.pt\n",
      "Epoch [8/50] - Train Loss: 0.5309 - Test Acc: 82.00%\n",
      "--> New best model saved (epoch 8, acc 82.00%) to: best_finetuned_model10.pt\n",
      "Epoch [9/50] - Train Loss: 0.5036 - Test Acc: 81.34%\n",
      "Epoch [10/50] - Train Loss: 0.4800 - Test Acc: 82.62%\n",
      "--> New best model saved (epoch 10, acc 82.62%) to: best_finetuned_model10.pt\n",
      "Epoch [11/50] - Train Loss: 0.4582 - Test Acc: 83.63%\n",
      "--> New best model saved (epoch 11, acc 83.63%) to: best_finetuned_model10.pt\n",
      "Epoch [12/50] - Train Loss: 0.4377 - Test Acc: 83.45%\n",
      "Epoch [13/50] - Train Loss: 0.4229 - Test Acc: 83.26%\n",
      "Epoch [14/50] - Train Loss: 0.4018 - Test Acc: 84.22%\n",
      "--> New best model saved (epoch 14, acc 84.22%) to: best_finetuned_model10.pt\n",
      "Epoch [15/50] - Train Loss: 0.3815 - Test Acc: 84.00%\n",
      "Epoch [16/50] - Train Loss: 0.3698 - Test Acc: 84.88%\n",
      "--> New best model saved (epoch 16, acc 84.88%) to: best_finetuned_model10.pt\n",
      "Epoch [17/50] - Train Loss: 0.3533 - Test Acc: 85.64%\n",
      "--> New best model saved (epoch 17, acc 85.64%) to: best_finetuned_model10.pt\n",
      "Epoch [18/50] - Train Loss: 0.3451 - Test Acc: 84.49%\n",
      "Epoch [19/50] - Train Loss: 0.3253 - Test Acc: 85.76%\n",
      "--> New best model saved (epoch 19, acc 85.76%) to: best_finetuned_model10.pt\n",
      "Epoch [20/50] - Train Loss: 0.3130 - Test Acc: 86.58%\n",
      "--> New best model saved (epoch 20, acc 86.58%) to: best_finetuned_model10.pt\n",
      "Epoch [21/50] - Train Loss: 0.3023 - Test Acc: 85.70%\n",
      "Epoch [22/50] - Train Loss: 0.2947 - Test Acc: 86.17%\n",
      "Epoch [23/50] - Train Loss: 0.2872 - Test Acc: 84.49%\n",
      "Epoch [24/50] - Train Loss: 0.2749 - Test Acc: 86.64%\n",
      "--> New best model saved (epoch 24, acc 86.64%) to: best_finetuned_model10.pt\n",
      "Epoch [25/50] - Train Loss: 0.2647 - Test Acc: 85.21%\n",
      "Epoch [26/50] - Train Loss: 0.2563 - Test Acc: 87.00%\n",
      "--> New best model saved (epoch 26, acc 87.00%) to: best_finetuned_model10.pt\n",
      "Epoch [27/50] - Train Loss: 0.2443 - Test Acc: 86.01%\n",
      "Epoch [28/50] - Train Loss: 0.2345 - Test Acc: 86.35%\n",
      "Epoch [29/50] - Train Loss: 0.2327 - Test Acc: 87.09%\n",
      "--> New best model saved (epoch 29, acc 87.09%) to: best_finetuned_model10.pt\n",
      "Epoch [30/50] - Train Loss: 0.2229 - Test Acc: 86.70%\n",
      "Epoch [31/50] - Train Loss: 0.2155 - Test Acc: 87.21%\n",
      "--> New best model saved (epoch 31, acc 87.21%) to: best_finetuned_model10.pt\n",
      "Epoch [32/50] - Train Loss: 0.2074 - Test Acc: 87.03%\n",
      "Epoch [33/50] - Train Loss: 0.2071 - Test Acc: 86.48%\n",
      "Epoch [34/50] - Train Loss: 0.2016 - Test Acc: 86.67%\n",
      "Epoch [35/50] - Train Loss: 0.1907 - Test Acc: 87.08%\n",
      "Epoch [36/50] - Train Loss: 0.1850 - Test Acc: 87.08%\n",
      "Epoch [37/50] - Train Loss: 0.1830 - Test Acc: 87.59%\n",
      "--> New best model saved (epoch 37, acc 87.59%) to: best_finetuned_model10.pt\n",
      "Epoch [38/50] - Train Loss: 0.1727 - Test Acc: 87.75%\n",
      "--> New best model saved (epoch 38, acc 87.75%) to: best_finetuned_model10.pt\n",
      "Epoch [39/50] - Train Loss: 0.1715 - Test Acc: 87.13%\n",
      "Epoch [40/50] - Train Loss: 0.1666 - Test Acc: 87.61%\n",
      "Epoch [41/50] - Train Loss: 0.1655 - Test Acc: 87.84%\n",
      "--> New best model saved (epoch 41, acc 87.84%) to: best_finetuned_model10.pt\n",
      "Epoch [42/50] - Train Loss: 0.1545 - Test Acc: 88.04%\n",
      "--> New best model saved (epoch 42, acc 88.04%) to: best_finetuned_model10.pt\n",
      "Epoch [43/50] - Train Loss: 0.1536 - Test Acc: 88.33%\n",
      "--> New best model saved (epoch 43, acc 88.33%) to: best_finetuned_model10.pt\n",
      "Epoch [44/50] - Train Loss: 0.1498 - Test Acc: 87.91%\n",
      "Epoch [45/50] - Train Loss: 0.1422 - Test Acc: 88.08%\n",
      "Epoch [46/50] - Train Loss: 0.1407 - Test Acc: 86.99%\n",
      "Epoch [47/50] - Train Loss: 0.1383 - Test Acc: 88.38%\n",
      "--> New best model saved (epoch 47, acc 88.38%) to: best_finetuned_model10.pt\n",
      "Epoch [48/50] - Train Loss: 0.1311 - Test Acc: 87.93%\n",
      "Epoch [49/50] - Train Loss: 0.1309 - Test Acc: 87.63%\n",
      "Epoch [50/50] - Train Loss: 0.1284 - Test Acc: 87.93%\n",
      "Finished fine-tuning. Best epoch: 47 with Test Acc: 88.38%\n",
      "Final fine-tuned model saved to: finetuned_model10.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_vgg16_bn\", pretrained=False)\n",
    "modelpath = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\intermediate_models\\stage3_trained_model.pt\"\n",
    "model.load_state_dict(torch.load(modelpath, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "try:\n",
    "    maskpath = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\intermediate_models\\stage3_mask.pt\"\n",
    "    final_mask = torch.load(maskpath, map_location=device)\n",
    "    from pruning.GraSP import apply_mask\n",
    "    apply_mask(model, final_mask)\n",
    "    print(\"Applied final pruning mask before fine-tuning.\")\n",
    "except FileNotFoundError:\n",
    "    final_mask = None\n",
    "    print(\"No final mask found. Continuing without reapplying mask.\")\n",
    "\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.reset_running_stats()\n",
    "\n",
    "CEloss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "best_ckpt_path = \"best_finetuned_model10.pt\"\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "    return acc\n",
    "\n",
    "def finetune(model, train_loader, test_loader, loss_fn, optimizer, epochs, device, mask=None, best_ckpt_path=best_ckpt_path):\n",
    "    best_acc = -1.0\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if mask is not None:\n",
    "                apply_mask(model, mask)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "        test_acc = evaluate(model, test_loader, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{epochs}] - Train Loss: {avg_loss:.4f} - Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_epoch = epoch\n",
    "\n",
    "            if mask is not None:\n",
    "                apply_mask(model, mask)\n",
    "\n",
    "            ckpt = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_acc': best_acc,\n",
    "            }\n",
    "            torch.save(ckpt, best_ckpt_path)\n",
    "            print(f\"--> New best model saved (epoch {epoch}, acc {best_acc:.2f}%) to: {best_ckpt_path}\")\n",
    "\n",
    "    print(f\"Finished fine-tuning. Best epoch: {best_epoch} with Test Acc: {best_acc:.2f}%\")\n",
    "    return best_epoch, best_acc\n",
    "\n",
    "best_epoch, best_acc = finetune(\n",
    "    model=model,\n",
    "    train_loader=train10,\n",
    "    test_loader=test10,\n",
    "    loss_fn=CEloss,\n",
    "    optimizer=optimizer,\n",
    "    epochs=50,\n",
    "    device=device,\n",
    "    mask=final_mask,\n",
    "    best_ckpt_path=best_ckpt_path\n",
    ")\n",
    "\n",
    "final_state_path = \"finetuned_model10.pt\"\n",
    "if final_mask is not None:\n",
    "    apply_mask(model, final_mask)\n",
    "torch.save(model.state_dict(), final_state_path)\n",
    "print(f\"Final fine-tuned model saved to: {final_state_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a20214",
   "metadata": {},
   "source": [
    "### cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0912210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "indices = random.sample(range(len(train100.dataset)), 500)\n",
    "subset_500 = Subset(train100.dataset, indices)\n",
    "subset_loader = torch.utils.data.DataLoader(subset_500, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef759e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_until(model, loss_fn, optimizer, target_acc, train_loader, test_loader, device, epochs=50):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for (x, y) in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for (x_val, y_val) in test_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                preds = model(x_val)\n",
    "                predicted = preds.argmax(dim=1)\n",
    "                correct += (predicted == y_val).sum().item()\n",
    "                total += y_val.size(0)\n",
    "        \n",
    "        val_acc = 100 * correct / total\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        if val_acc >= target_acc:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (val acc = {val_acc:.2f}%)\")\n",
    "            break\n",
    "    return model\n",
    "\n",
    "def train_until_masked(model, loss_fn, optimizer, target_acc, train_loader, test_loader, device, mask_dict, epochs=50):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for (x, y) in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                for name, param in model.named_parameters():\n",
    "                    if name in mask_dict:\n",
    "                        param.mul_(mask_dict[name]) \n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for (x_val, y_val) in test_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                preds = model(x_val)\n",
    "                predicted = preds.argmax(dim=1)\n",
    "                correct += (predicted == y_val).sum().item()\n",
    "                total += y_val.size(0)\n",
    "        \n",
    "        val_acc = 100 * correct / total\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        if val_acc >= target_acc:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (val acc = {val_acc:.2f}%)\")\n",
    "            break\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d77e682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stage 0] Training CIFAR-100 model to ~20% accuracy\n",
      "Epoch [1/50] - Loss: 0.3833 | Val Acc: 60.84%\n",
      "Stopping early at epoch 1 (val acc = 60.84%)\n",
      "Saved Stage 0 trained model for CIFAR-100.\n",
      "\n",
      "\n",
      "=== Pruning Stage 1 ===\n",
      "Target sparsity: 28.0%\n",
      "→ Computing saliency scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_5916\\2580704963.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model100.load_state_dict(torch.load(prev_stage_model, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rank_by_saliency] Pruned 4283929 / 15299748 (target 4283929). Kept 11015819 params. threshold=2.21005e-05\n",
      "→ Applied pruning mask up to 28.0% sparsity (threshold=2.2100510250311345e-05).\n",
      "Epoch [1/50] - Loss: 0.5293 | Val Acc: 57.32%\n",
      "Epoch [2/50] - Loss: 0.5029 | Val Acc: 56.45%\n",
      "Epoch [3/50] - Loss: 0.4974 | Val Acc: 58.46%\n",
      "Epoch [4/50] - Loss: 0.4696 | Val Acc: 57.89%\n",
      "Epoch [5/50] - Loss: 0.4567 | Val Acc: 58.67%\n",
      "Epoch [6/50] - Loss: 0.4467 | Val Acc: 57.95%\n",
      "Epoch [7/50] - Loss: 0.4358 | Val Acc: 58.14%\n",
      "Epoch [8/50] - Loss: 0.4184 | Val Acc: 57.90%\n",
      "Epoch [9/50] - Loss: 0.4161 | Val Acc: 58.83%\n",
      "Epoch [10/50] - Loss: 0.3913 | Val Acc: 58.91%\n",
      "Epoch [11/50] - Loss: 0.3923 | Val Acc: 58.69%\n",
      "Epoch [12/50] - Loss: 0.3953 | Val Acc: 58.01%\n",
      "Epoch [13/50] - Loss: 0.3807 | Val Acc: 58.40%\n",
      "Epoch [14/50] - Loss: 0.3671 | Val Acc: 58.38%\n",
      "Epoch [15/50] - Loss: 0.3660 | Val Acc: 58.47%\n",
      "Epoch [16/50] - Loss: 0.3633 | Val Acc: 58.38%\n",
      "Epoch [17/50] - Loss: 0.3494 | Val Acc: 59.71%\n",
      "Epoch [18/50] - Loss: 0.3472 | Val Acc: 58.69%\n",
      "Epoch [19/50] - Loss: 0.3395 | Val Acc: 59.15%\n",
      "Epoch [20/50] - Loss: 0.3250 | Val Acc: 59.78%\n",
      "Epoch [21/50] - Loss: 0.3265 | Val Acc: 59.04%\n",
      "Epoch [22/50] - Loss: 0.3169 | Val Acc: 59.23%\n",
      "Epoch [23/50] - Loss: 0.3198 | Val Acc: 59.15%\n",
      "Epoch [24/50] - Loss: 0.3101 | Val Acc: 58.99%\n",
      "Epoch [25/50] - Loss: 0.3160 | Val Acc: 58.96%\n",
      "Epoch [26/50] - Loss: 0.2951 | Val Acc: 59.55%\n",
      "Epoch [27/50] - Loss: 0.2929 | Val Acc: 59.75%\n",
      "Epoch [28/50] - Loss: 0.2894 | Val Acc: 58.75%\n",
      "Epoch [29/50] - Loss: 0.2862 | Val Acc: 59.81%\n",
      "Epoch [30/50] - Loss: 0.2709 | Val Acc: 59.78%\n",
      "Epoch [31/50] - Loss: 0.2845 | Val Acc: 59.87%\n",
      "Epoch [32/50] - Loss: 0.2641 | Val Acc: 59.81%\n",
      "Epoch [33/50] - Loss: 0.2761 | Val Acc: 59.85%\n",
      "Epoch [34/50] - Loss: 0.2614 | Val Acc: 59.38%\n",
      "Epoch [35/50] - Loss: 0.2530 | Val Acc: 59.10%\n",
      "Epoch [36/50] - Loss: 0.2553 | Val Acc: 59.21%\n",
      "Epoch [37/50] - Loss: 0.2519 | Val Acc: 59.76%\n",
      "Epoch [38/50] - Loss: 0.2518 | Val Acc: 60.09%\n",
      "Stopping early at epoch 38 (val acc = 60.09%)\n",
      "Stage 1 complete and saved.\n",
      "\n",
      "=== Pruning Stage 2 ===\n",
      "Target sparsity: 45.5%\n",
      "→ Computing saliency scores...\n",
      "[rank_by_saliency] Pruned 6961385 / 15299748 (target 6961385). Kept 8338363 params. threshold=0.0031645\n",
      "→ Applied pruning mask up to 45.5% sparsity (threshold=0.0031644990667700768).\n",
      "Epoch [1/50] - Loss: 0.3264 | Val Acc: 58.97%\n",
      "Epoch [2/50] - Loss: 0.2847 | Val Acc: 60.09%\n",
      "Epoch [3/50] - Loss: 0.2547 | Val Acc: 60.42%\n",
      "Epoch [4/50] - Loss: 0.2538 | Val Acc: 60.23%\n",
      "Epoch [5/50] - Loss: 0.2487 | Val Acc: 60.53%\n",
      "Epoch [6/50] - Loss: 0.2301 | Val Acc: 60.51%\n",
      "Epoch [7/50] - Loss: 0.2261 | Val Acc: 60.65%\n",
      "Epoch [8/50] - Loss: 0.2263 | Val Acc: 60.43%\n",
      "Epoch [9/50] - Loss: 0.2255 | Val Acc: 60.49%\n",
      "Epoch [10/50] - Loss: 0.2201 | Val Acc: 60.44%\n",
      "Epoch [11/50] - Loss: 0.2170 | Val Acc: 59.27%\n",
      "Epoch [12/50] - Loss: 0.2100 | Val Acc: 60.31%\n",
      "Epoch [13/50] - Loss: 0.2106 | Val Acc: 61.08%\n",
      "Epoch [14/50] - Loss: 0.2114 | Val Acc: 60.16%\n",
      "Epoch [15/50] - Loss: 0.1956 | Val Acc: 61.12%\n",
      "Epoch [16/50] - Loss: 0.1940 | Val Acc: 60.59%\n",
      "Epoch [17/50] - Loss: 0.1992 | Val Acc: 59.63%\n",
      "Epoch [18/50] - Loss: 0.1925 | Val Acc: 60.68%\n",
      "Epoch [19/50] - Loss: 0.2019 | Val Acc: 60.21%\n",
      "Epoch [20/50] - Loss: 0.1707 | Val Acc: 60.77%\n",
      "Epoch [21/50] - Loss: 0.1847 | Val Acc: 61.55%\n",
      "Epoch [22/50] - Loss: 0.1894 | Val Acc: 60.99%\n",
      "Epoch [23/50] - Loss: 0.1769 | Val Acc: 60.40%\n",
      "Epoch [24/50] - Loss: 0.1765 | Val Acc: 60.47%\n",
      "Epoch [25/50] - Loss: 0.1802 | Val Acc: 60.68%\n",
      "Epoch [26/50] - Loss: 0.1825 | Val Acc: 61.40%\n",
      "Epoch [27/50] - Loss: 0.1713 | Val Acc: 60.34%\n",
      "Epoch [28/50] - Loss: 0.1772 | Val Acc: 61.01%\n",
      "Epoch [29/50] - Loss: 0.1745 | Val Acc: 60.70%\n",
      "Epoch [30/50] - Loss: 0.1660 | Val Acc: 61.15%\n",
      "Epoch [31/50] - Loss: 0.1533 | Val Acc: 60.89%\n",
      "Epoch [32/50] - Loss: 0.1677 | Val Acc: 61.38%\n",
      "Epoch [33/50] - Loss: 0.1670 | Val Acc: 61.79%\n",
      "Epoch [34/50] - Loss: 0.1603 | Val Acc: 61.48%\n",
      "Epoch [35/50] - Loss: 0.1610 | Val Acc: 60.25%\n",
      "Epoch [36/50] - Loss: 0.1558 | Val Acc: 61.14%\n",
      "Epoch [37/50] - Loss: 0.1593 | Val Acc: 61.80%\n",
      "Epoch [38/50] - Loss: 0.1509 | Val Acc: 60.89%\n",
      "Epoch [39/50] - Loss: 0.1528 | Val Acc: 60.64%\n",
      "Epoch [40/50] - Loss: 0.1593 | Val Acc: 61.05%\n",
      "Epoch [41/50] - Loss: 0.1449 | Val Acc: 60.91%\n",
      "Epoch [42/50] - Loss: 0.1500 | Val Acc: 61.22%\n",
      "Epoch [43/50] - Loss: 0.1526 | Val Acc: 61.67%\n",
      "Epoch [44/50] - Loss: 0.1470 | Val Acc: 61.24%\n",
      "Epoch [45/50] - Loss: 0.1418 | Val Acc: 61.16%\n",
      "Epoch [46/50] - Loss: 0.1556 | Val Acc: 60.78%\n",
      "Epoch [47/50] - Loss: 0.1522 | Val Acc: 61.50%\n",
      "Epoch [48/50] - Loss: 0.1445 | Val Acc: 61.67%\n",
      "Epoch [49/50] - Loss: 0.1365 | Val Acc: 61.24%\n",
      "Epoch [50/50] - Loss: 0.1354 | Val Acc: 62.06%\n",
      "Stage 2 complete and saved.\n",
      "\n",
      "=== Pruning Stage 3 ===\n",
      "Target sparsity: 70.0%\n",
      "→ Computing saliency scores...\n",
      "[rank_by_saliency] Pruned 10709824 / 15299748 (target 10709824). Kept 4589924 params. threshold=0.0315055\n",
      "→ Applied pruning mask up to 70.0% sparsity (threshold=0.031505465507507324).\n",
      "Stage 3 complete and saved.\n",
      "\n",
      "=== Final Profiling ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_5916\\2580704963.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model100.load_state_dict(torch.load(f\"stage{len(stage_fractions)}_trained_model100.pt\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model100 = model100.to(device)\n",
    "\n",
    "initial_target_acc = 50\n",
    "final_target_sparsity = 0.7\n",
    "stage_fractions = [0.4, 0.65, 1.0] \n",
    "target_accuracies = [60, 70]      \n",
    "epochs_per_stage = 50\n",
    "\n",
    "CEloss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_100 = torch.optim.Adam(model100.parameters(), lr=1e-5)\n",
    "\n",
    "print(\"\\n[Stage 0] Training CIFAR-100 model to ~20% accuracy\")\n",
    "model100 = train_until(\n",
    "    model=model100,\n",
    "    loss_fn=CEloss,\n",
    "    optimizer=optimizer_100,\n",
    "    target_acc=initial_target_acc,\n",
    "    train_loader=train100, \n",
    "    test_loader=test100,   \n",
    "    device=device,\n",
    "    epochs=epochs_per_stage\n",
    ")\n",
    "torch.save(model100.state_dict(), \"stage0_trained_model100.pt\")\n",
    "print(\"Saved Stage 0 trained model for CIFAR-100.\\n\")\n",
    "\n",
    "current_mask = None\n",
    "current_sparsity = 0.0\n",
    "\n",
    "for stage_idx, fraction in enumerate(stage_fractions):\n",
    "    target_sparsity = fraction * final_target_sparsity\n",
    "    print(f\"\\n=== Pruning Stage {stage_idx + 1} ===\")\n",
    "    print(f\"Target sparsity: {target_sparsity*100:.1f}%\")\n",
    "    \n",
    "    prev_stage_model = f\"stage{stage_idx}_trained_model100.pt\" if stage_idx > 0 else \"stage0_trained_model100.pt\"\n",
    "    model100.load_state_dict(torch.load(prev_stage_model, map_location=device))\n",
    "    model100.to(device)\n",
    "\n",
    "    print(\"→ Computing saliency scores...\")\n",
    "    scores = saliency_scores(model100, subset_loader, device, CEloss) \n",
    "\n",
    "    mask, thresh = rank_by_saliency(\n",
    "        scores=scores,\n",
    "        current_mask=current_mask,\n",
    "        current_sparsity=current_sparsity,\n",
    "        target_sparsity=target_sparsity\n",
    "    )\n",
    "\n",
    "    if mask is not None:\n",
    "        apply_mask(model100, mask)\n",
    "        print(f\"→ Applied pruning mask up to {target_sparsity*100:.1f}% sparsity (threshold={thresh}).\")\n",
    "\n",
    "    for m in model100.modules():\n",
    "        if isinstance(m, torch.nn.BatchNorm2d):\n",
    "            m.reset_running_stats()\n",
    "\n",
    "    if stage_idx < len(stage_fractions) - 1:\n",
    "        optimizer_stage = torch.optim.Adam(model100.parameters(), lr=1e-4)\n",
    "        model100 = train_until_masked(\n",
    "            model=model100,\n",
    "            loss_fn=CEloss,\n",
    "            optimizer=optimizer_stage, \n",
    "            target_acc=target_accuracies[stage_idx],\n",
    "            train_loader=train100,\n",
    "            test_loader=test100,  \n",
    "            device=device,\n",
    "            mask_dict=mask if mask is not None else current_mask,\n",
    "            epochs=epochs_per_stage\n",
    "        )\n",
    "    torch.save(model100.state_dict(), f\"stage{stage_idx+1}_trained_model100.pt\")\n",
    "    if mask is not None:\n",
    "        torch.save(mask, f\"stage{stage_idx+1}_mask100.pt\")\n",
    "        current_mask = mask\n",
    "    current_sparsity = target_sparsity\n",
    "    print(f\"Stage {stage_idx + 1} complete and saved.\")\n",
    "print(\"\\n=== Final Profiling ===\\n\")\n",
    "model100.load_state_dict(torch.load(f\"stage{len(stage_fractions)}_trained_model100.pt\", map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a05f736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Fatim_Sproj/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n",
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_5916\\2845645996.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(modelpath, map_location=device))\n",
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_5916\\2845645996.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  final_mask = torch.load(maskpath, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied final pruning mask before fine-tuning.\n",
      "Epoch [1/80] - Train Loss: 2.7483 - Test Acc: 36.06%\n",
      "--> New best model saved (epoch 1, acc 36.06%) to: best_finetuned_model100.pt\n",
      "Epoch [2/80] - Train Loss: 2.4600 - Test Acc: 40.27%\n",
      "--> New best model saved (epoch 2, acc 40.27%) to: best_finetuned_model100.pt\n",
      "Epoch [3/80] - Train Loss: 2.3003 - Test Acc: 41.16%\n",
      "--> New best model saved (epoch 3, acc 41.16%) to: best_finetuned_model100.pt\n",
      "Epoch [4/80] - Train Loss: 2.1902 - Test Acc: 43.07%\n",
      "--> New best model saved (epoch 4, acc 43.07%) to: best_finetuned_model100.pt\n",
      "Epoch [5/80] - Train Loss: 2.1164 - Test Acc: 43.83%\n",
      "--> New best model saved (epoch 5, acc 43.83%) to: best_finetuned_model100.pt\n",
      "Epoch [6/80] - Train Loss: 2.0095 - Test Acc: 44.61%\n",
      "--> New best model saved (epoch 6, acc 44.61%) to: best_finetuned_model100.pt\n",
      "Epoch [7/80] - Train Loss: 1.9513 - Test Acc: 42.79%\n",
      "Epoch [8/80] - Train Loss: 1.8848 - Test Acc: 47.32%\n",
      "--> New best model saved (epoch 8, acc 47.32%) to: best_finetuned_model100.pt\n",
      "Epoch [9/80] - Train Loss: 1.8126 - Test Acc: 48.30%\n",
      "--> New best model saved (epoch 9, acc 48.30%) to: best_finetuned_model100.pt\n",
      "Epoch [10/80] - Train Loss: 1.7700 - Test Acc: 48.85%\n",
      "--> New best model saved (epoch 10, acc 48.85%) to: best_finetuned_model100.pt\n",
      "Epoch [11/80] - Train Loss: 1.7244 - Test Acc: 49.34%\n",
      "--> New best model saved (epoch 11, acc 49.34%) to: best_finetuned_model100.pt\n",
      "Epoch [12/80] - Train Loss: 1.6708 - Test Acc: 49.15%\n",
      "Epoch [13/80] - Train Loss: 1.6435 - Test Acc: 50.52%\n",
      "--> New best model saved (epoch 13, acc 50.52%) to: best_finetuned_model100.pt\n",
      "Epoch [14/80] - Train Loss: 1.6016 - Test Acc: 49.48%\n",
      "Epoch [15/80] - Train Loss: 1.5659 - Test Acc: 51.04%\n",
      "--> New best model saved (epoch 15, acc 51.04%) to: best_finetuned_model100.pt\n",
      "Epoch [16/80] - Train Loss: 1.5332 - Test Acc: 51.42%\n",
      "--> New best model saved (epoch 16, acc 51.42%) to: best_finetuned_model100.pt\n",
      "Epoch [17/80] - Train Loss: 1.4967 - Test Acc: 51.64%\n",
      "--> New best model saved (epoch 17, acc 51.64%) to: best_finetuned_model100.pt\n",
      "Epoch [18/80] - Train Loss: 1.4638 - Test Acc: 51.22%\n",
      "Epoch [19/80] - Train Loss: 1.4455 - Test Acc: 51.97%\n",
      "--> New best model saved (epoch 19, acc 51.97%) to: best_finetuned_model100.pt\n",
      "Epoch [20/80] - Train Loss: 1.4029 - Test Acc: 51.29%\n",
      "Epoch [21/80] - Train Loss: 1.3983 - Test Acc: 52.62%\n",
      "--> New best model saved (epoch 21, acc 52.62%) to: best_finetuned_model100.pt\n",
      "Epoch [22/80] - Train Loss: 1.3663 - Test Acc: 52.44%\n",
      "Epoch [23/80] - Train Loss: 1.3463 - Test Acc: 51.75%\n",
      "Epoch [24/80] - Train Loss: 1.3038 - Test Acc: 52.44%\n",
      "Epoch [25/80] - Train Loss: 1.3019 - Test Acc: 52.03%\n",
      "Epoch [26/80] - Train Loss: 1.2737 - Test Acc: 52.72%\n",
      "--> New best model saved (epoch 26, acc 52.72%) to: best_finetuned_model100.pt\n",
      "Epoch [27/80] - Train Loss: 1.2405 - Test Acc: 53.53%\n",
      "--> New best model saved (epoch 27, acc 53.53%) to: best_finetuned_model100.pt\n",
      "Epoch [28/80] - Train Loss: 1.2217 - Test Acc: 51.69%\n",
      "Epoch [29/80] - Train Loss: 1.2201 - Test Acc: 53.04%\n",
      "Epoch [30/80] - Train Loss: 1.1948 - Test Acc: 52.93%\n",
      "Epoch [31/80] - Train Loss: 1.1719 - Test Acc: 53.22%\n",
      "Epoch [32/80] - Train Loss: 1.1597 - Test Acc: 53.78%\n",
      "--> New best model saved (epoch 32, acc 53.78%) to: best_finetuned_model100.pt\n",
      "Epoch [33/80] - Train Loss: 1.1468 - Test Acc: 53.48%\n",
      "Epoch [34/80] - Train Loss: 1.1186 - Test Acc: 51.75%\n",
      "Epoch [35/80] - Train Loss: 1.1232 - Test Acc: 52.84%\n",
      "Epoch [36/80] - Train Loss: 1.1277 - Test Acc: 53.93%\n",
      "--> New best model saved (epoch 36, acc 53.93%) to: best_finetuned_model100.pt\n",
      "Epoch [37/80] - Train Loss: 1.0860 - Test Acc: 53.63%\n",
      "Epoch [38/80] - Train Loss: 1.0900 - Test Acc: 53.77%\n",
      "Epoch [39/80] - Train Loss: 1.0671 - Test Acc: 53.52%\n",
      "Epoch [40/80] - Train Loss: 1.0566 - Test Acc: 54.28%\n",
      "--> New best model saved (epoch 40, acc 54.28%) to: best_finetuned_model100.pt\n",
      "Epoch [41/80] - Train Loss: 1.0563 - Test Acc: 54.75%\n",
      "--> New best model saved (epoch 41, acc 54.75%) to: best_finetuned_model100.pt\n",
      "Epoch [42/80] - Train Loss: 1.0345 - Test Acc: 53.86%\n",
      "Epoch [43/80] - Train Loss: 1.0310 - Test Acc: 54.51%\n",
      "Epoch [44/80] - Train Loss: 1.0028 - Test Acc: 54.35%\n",
      "Epoch [45/80] - Train Loss: 1.0148 - Test Acc: 54.01%\n",
      "Epoch [46/80] - Train Loss: 0.9878 - Test Acc: 54.45%\n",
      "Epoch [47/80] - Train Loss: 0.9791 - Test Acc: 54.91%\n",
      "--> New best model saved (epoch 47, acc 54.91%) to: best_finetuned_model100.pt\n",
      "Epoch [48/80] - Train Loss: 0.9867 - Test Acc: 53.57%\n",
      "Epoch [49/80] - Train Loss: 0.9712 - Test Acc: 54.39%\n",
      "Epoch [50/80] - Train Loss: 0.9737 - Test Acc: 55.21%\n",
      "--> New best model saved (epoch 50, acc 55.21%) to: best_finetuned_model100.pt\n",
      "Epoch [51/80] - Train Loss: 0.9458 - Test Acc: 55.24%\n",
      "--> New best model saved (epoch 51, acc 55.24%) to: best_finetuned_model100.pt\n",
      "Epoch [52/80] - Train Loss: 0.9484 - Test Acc: 54.52%\n",
      "Epoch [53/80] - Train Loss: 0.9412 - Test Acc: 54.31%\n",
      "Epoch [54/80] - Train Loss: 0.9405 - Test Acc: 54.28%\n",
      "Epoch [55/80] - Train Loss: 0.9299 - Test Acc: 54.91%\n",
      "Epoch [56/80] - Train Loss: 0.9394 - Test Acc: 55.22%\n",
      "Epoch [57/80] - Train Loss: 0.9193 - Test Acc: 55.23%\n",
      "Epoch [58/80] - Train Loss: 0.9101 - Test Acc: 55.02%\n",
      "Epoch [59/80] - Train Loss: 0.9016 - Test Acc: 54.29%\n",
      "Epoch [60/80] - Train Loss: 0.9011 - Test Acc: 54.55%\n",
      "Epoch [61/80] - Train Loss: 0.8995 - Test Acc: 54.26%\n",
      "Epoch [62/80] - Train Loss: 0.9144 - Test Acc: 55.96%\n",
      "--> New best model saved (epoch 62, acc 55.96%) to: best_finetuned_model100.pt\n",
      "Epoch [63/80] - Train Loss: 0.8871 - Test Acc: 54.91%\n",
      "Epoch [64/80] - Train Loss: 0.8744 - Test Acc: 55.38%\n",
      "Epoch [65/80] - Train Loss: 0.8760 - Test Acc: 54.87%\n",
      "Epoch [66/80] - Train Loss: 0.8790 - Test Acc: 53.82%\n",
      "Epoch [67/80] - Train Loss: 0.8691 - Test Acc: 55.61%\n",
      "Epoch [68/80] - Train Loss: 0.8564 - Test Acc: 54.42%\n",
      "Epoch [69/80] - Train Loss: 0.8667 - Test Acc: 54.63%\n",
      "Epoch [70/80] - Train Loss: 0.8742 - Test Acc: 54.82%\n",
      "Epoch [71/80] - Train Loss: 0.8625 - Test Acc: 54.76%\n",
      "Epoch [72/80] - Train Loss: 0.8559 - Test Acc: 55.28%\n",
      "Epoch [73/80] - Train Loss: 0.8406 - Test Acc: 54.27%\n",
      "Epoch [74/80] - Train Loss: 0.8602 - Test Acc: 54.74%\n",
      "Epoch [75/80] - Train Loss: 0.8529 - Test Acc: 54.61%\n",
      "Epoch [76/80] - Train Loss: 0.8515 - Test Acc: 55.43%\n",
      "Epoch [77/80] - Train Loss: 0.8244 - Test Acc: 54.50%\n",
      "Epoch [78/80] - Train Loss: 0.8323 - Test Acc: 54.68%\n",
      "Epoch [79/80] - Train Loss: 0.8553 - Test Acc: 55.21%\n",
      "Epoch [80/80] - Train Loss: 0.8218 - Test Acc: 55.29%\n",
      "Finished fine-tuning. Best epoch: 62 with Test Acc: 55.96%\n",
      "Final fine-tuned model saved to: finetuned_model100.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar100_vgg16_bn\", pretrained=False)\n",
    "modelpath = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\intermediate_models\\stage3_trained_model100.pt\"\n",
    "model.load_state_dict(torch.load(modelpath, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "try:\n",
    "    maskpath = r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\aiedge\\Pruning\\intermediate_models\\stage3_mask100.pt\"\n",
    "    final_mask = torch.load(maskpath, map_location=device)\n",
    "    from pruning.GraSP import apply_mask\n",
    "    apply_mask(model, final_mask)\n",
    "    print(\"Applied final pruning mask before fine-tuning.\")\n",
    "except FileNotFoundError:\n",
    "    final_mask = None\n",
    "    print(\"No final mask found. Continuing without reapplying mask.\")\n",
    "\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.reset_running_stats()\n",
    "\n",
    "CEloss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "best_ckpt_path = \"best_finetuned_model100.pt\"\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "    return acc\n",
    "\n",
    "def finetune(model, train_loader, test_loader, loss_fn, optimizer, epochs, device, mask=None, best_ckpt_path=best_ckpt_path):\n",
    "    best_acc = -1.0\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if mask is not None:\n",
    "                apply_mask(model, mask)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "        test_acc = evaluate(model, test_loader, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{epochs}] - Train Loss: {avg_loss:.4f} - Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_epoch = epoch\n",
    "\n",
    "            if mask is not None:\n",
    "                apply_mask(model, mask)\n",
    "\n",
    "            ckpt = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_acc': best_acc,\n",
    "            }\n",
    "            torch.save(ckpt, best_ckpt_path)\n",
    "            print(f\"--> New best model saved (epoch {epoch}, acc {best_acc:.2f}%) to: {best_ckpt_path}\")\n",
    "\n",
    "    print(f\"Finished fine-tuning. Best epoch: {best_epoch} with Test Acc: {best_acc:.2f}%\")\n",
    "    return best_epoch, best_acc\n",
    "\n",
    "best_epoch, best_acc = finetune(\n",
    "    model=model,\n",
    "    train_loader=train100,\n",
    "    test_loader=test100,\n",
    "    loss_fn=CEloss,\n",
    "    optimizer=optimizer,\n",
    "    epochs=80,\n",
    "    device=device,\n",
    "    mask=final_mask,\n",
    "    best_ckpt_path=best_ckpt_path\n",
    ")\n",
    "\n",
    "final_state_path = \"finetuned_model100.pt\"\n",
    "if final_mask is not None:\n",
    "    apply_mask(model, final_mask)\n",
    "torch.save(model.state_dict(), final_state_path)\n",
    "print(f\"Final fine-tuned model saved to: {final_state_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bacp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
